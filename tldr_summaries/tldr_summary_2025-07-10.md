# TLDR Newsletter Summary: 2025-07-10

## TLDR2025-07-10

### ⚡Warp's
 AI coding agent leaps ahead of Claude Code to hit #1 on Terminal-Bench (Sponsor)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.warp.dev%2F%3Futm_source=publications%26utm_medium=newsletter%26utm_campaign=7_10_2025_primary%26utm_content=tldr_ai/2/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/nBzH3wXBs0YDv5B_mC5O6VluoqqRn7-DBVmFWNqKKy4=413
**TLDR Summary:** Warp just launched the first Agentic Development Environment , built for devs who want to move faster. It's the top overall coding agent which just jumped ahead of Claude Code by 20% to become the #1 agent on Terminal-Bench and scored 71% on SWE-bench Verified . ✅ Long-running commands: something no other tool can support ✅ Agent multi-threading: run multiple agents in parallel – all under your control ✅ Across the development lifecycle: setup → coding → deployment Warp is emerging as the ADE of choice for pro's, trusted by 500K+ developers and 56% of the Fortune 500. “Absolutely amazing. Transformed my workflow.” — Yash Patil, Technical Staff at OpenAI Try Warp's coding agent for yourself →
**Full Article Content:**
“Warp's AI features are thoughtful and feel like part of the core experience, not an afterthought or add on. Warp’s coding agent works alongside you, for you, doubling my output and letting me multitask.”

---

## Big Tech & Startups

### xAI
 gave us early access to Grok 4 - and the results are in (4 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1943166841150644622.html%3Futm_source=tldrnewsletter/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/eUyeeSwWWPsu1IOvzeUMNYyTxhmteNRj_LorVz0J1T0=413
**TLDR Summary:** According to new benchmark tests, Grok 4 is now the leading AI model. Grok 4 is a reasoning model that costs $3 per million input tokens and $15 per million output tokens. It is expected to be available via the xAI API, the Grok chatbot on X, and potentially on Microsoft Azure AI Foundry. xAI's API serves Grok 4 at 75 tokens per second, which is slower than OpenAI's o3 but faster than Anthropic's Claude 4 Opus Thinking.
**Full Article Content:**
DeepSeek’s R1 leaps over xAI, Meta and Anthropic to be tied as the world’s #2 AI Lab and the undisputed open-weights leaderDeepSeek R1 0528 has jumped from 60 to 68 in the Artificial Analysis Intelligence Index, our index of 7 leading evaluations that we run independently across all leading models. That’s the same magnitude of increase as the difference between OpenAI’s o1 and o3 (62 to 70).This positions DeepSeek R1 as higher intelligence than xAI’s Grok 3 mini (high), NVIDIA’s Llama Nemotron Ultra, Meta’s Llama 4 Maverick, Alibaba’s Qwen 3 253 and equal to Google’s Gemini 2.5 Pro.Breakdown of the model’s improvement:🧠 Intelligence increases across the board: Biggest jumps seen in AIME 2024 (Competition Math, +21 points), LiveCodeBench (Code generation, +15 points), GPQA Diamond (Scientific Reasoning, +10 points) and Humanity’s Last Exam (Reasoning & Knowledge, +6 points)🏠 No change to architecture: R1-0528 is a post-training update with no change to the V3/R1 architecture - it remains a large 671B model with 37B active parameters🧑‍💻 Significant leap in coding skills: R1 is now matching Gemini 2.5 Pro in the Artificial Analysis Coding Index and is behind only o4-mini (high) and o3🗯️ Increased token usage: R1-0528 used 99 million tokens to complete the evals in Artificial Analysis Intelligence Index, 40% more than the original R1’s 71 million tokens - ie. the new R1 thinks for longer than the original R1. This is still not the highest token usage number we have seen: Gemini 2.5 Pro is using 30% more tokens than R1-0528Takeaways for AI:👐 The gap between open and closed models is smaller than ever: open weights models have continued to maintain intelligence gains in-line with proprietary models. DeepSeek’s R1 release in January was the first time an open-weights model achieved the #2 position and DeepSeek’s R1 update today brings it back to the same position🇨🇳 China remains neck and neck with the US: models from China-based AI Labs have all but completely caught up to their US counterparts, this release continues the emerging trend. As of today, DeepSeek leads US based AI labs including Anthropic and Meta in Artificial Analysis Intelligence Index🔄 Improvements driven by reinforcement learning: DeepSeek has shown substantial intelligence improvements with the same architecture and pre-train as their original DeepSeek R1 release. This highlights the continually increasing importance of post-training, particularly for reasoning models trained with reinforcement learning (RL) techniques. OpenAI disclosed a 10x scaling of RL compute between o1 and o3 - DeepSeek have just demonstrated that so far, they can keep up with OpenAI’s RL compute scaling. Scaling RL demands less compute than scaling pre-training and offers an efficient way of achieving intelligence gains, supporting AI Labs with fewer GPUsSee further analysis below 👇

---

### xAI
 gave us early access to Grok 4 - and the results are in (4 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1943166841150644622.html%3Futm_source=tldrnewsletter/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/eUyeeSwWWPsu1IOvzeUMNYyTxhmteNRj_LorVz0J1T0=413
**TLDR Summary:** According to new benchmark tests, Grok 4 is now the leading AI model. Grok 4 is a reasoning model that costs $3 per million input tokens and $15 per million output tokens. It is expected to be available via the xAI API, the Grok chatbot on X, and potentially on Microsoft Azure AI Foundry. xAI's API serves Grok 4 at 75 tokens per second, which is slower than OpenAI's o3 but faster than Anthropic's Claude 4 Opus Thinking.
**Full Article Content:**
DeepSeek’s R1 leaps over xAI, Meta and Anthropic to be tied as the world’s #2 AI Lab and the undisputed open-weights leaderDeepSeek R1 0528 has jumped from 60 to 68 in the Artificial Analysis Intelligence Index, our index of 7 leading evaluations that we run independently across all leading models. That’s the same magnitude of increase as the difference between OpenAI’s o1 and o3 (62 to 70).This positions DeepSeek R1 as higher intelligence than xAI’s Grok 3 mini (high), NVIDIA’s Llama Nemotron Ultra, Meta’s Llama 4 Maverick, Alibaba’s Qwen 3 253 and equal to Google’s Gemini 2.5 Pro.Breakdown of the model’s improvement:🧠 Intelligence increases across the board: Biggest jumps seen in AIME 2024 (Competition Math, +21 points), LiveCodeBench (Code generation, +15 points), GPQA Diamond (Scientific Reasoning, +10 points) and Humanity’s Last Exam (Reasoning & Knowledge, +6 points)🏠 No change to architecture: R1-0528 is a post-training update with no change to the V3/R1 architecture - it remains a large 671B model with 37B active parameters🧑‍💻 Significant leap in coding skills: R1 is now matching Gemini 2.5 Pro in the Artificial Analysis Coding Index and is behind only o4-mini (high) and o3🗯️ Increased token usage: R1-0528 used 99 million tokens to complete the evals in Artificial Analysis Intelligence Index, 40% more than the original R1’s 71 million tokens - ie. the new R1 thinks for longer than the original R1. This is still not the highest token usage number we have seen: Gemini 2.5 Pro is using 30% more tokens than R1-0528Takeaways for AI:👐 The gap between open and closed models is smaller than ever: open weights models have continued to maintain intelligence gains in-line with proprietary models. DeepSeek’s R1 release in January was the first time an open-weights model achieved the #2 position and DeepSeek’s R1 update today brings it back to the same position🇨🇳 China remains neck and neck with the US: models from China-based AI Labs have all but completely caught up to their US counterparts, this release continues the emerging trend. As of today, DeepSeek leads US based AI labs including Anthropic and Meta in Artificial Analysis Intelligence Index🔄 Improvements driven by reinforcement learning: DeepSeek has shown substantial intelligence improvements with the same architecture and pre-train as their original DeepSeek R1 release. This highlights the continually increasing importance of post-training, particularly for reasoning models trained with reinforcement learning (RL) techniques. OpenAI disclosed a 10x scaling of RL compute between o1 and o3 - DeepSeek have just demonstrated that so far, they can keep up with OpenAI’s RL compute scaling. Scaling RL demands less compute than scaling pre-training and offers an efficient way of achieving intelligence gains, supporting AI Labs with fewer GPUsSee further analysis below 👇

---

### Linda
 Yaccarino Steps Down as CEO of Elon Musk's X (5 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FPlnFeN/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/cgVlYXzoP71LbiFKqRZlPNd4oieROfmAviK5oXU-61A=413
**TLDR Summary:** Linda Yaccarino is stepping down as chief executive of X after months of uncertainty at the company. The company initially struggled with an exodus of advertisers under Elon Musk's ownership. X is now expected to see ad revenue growth for the first time since the takeover. Yaccarino said that this made it a good time to depart.
**Full Article Content:**
[Scraping failed for this URL. Error: Article `download()` failed with 401 Client Error: HTTP Forbidden for url: https://www.wsj.com/tech/linda-yaccarino-x-ceo-steps-down-1550842e?mod=tech_trendingnow_article_pos1&utm_source=tldrnewsletter on URL https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FPlnFeN/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/cgVlYXzoP71LbiFKqRZlPNd4oieROfmAviK5oXU-61A=413]

---

## Science & Futuristic Technology

### Hugging
 Face just launched a $299 robot that could disrupt the entire robotics industry (13 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FjeLOBW/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/x--ZowRSzvF9cP_0WHfiJE5ZDMhyDFuJ0nxB2M_fFBU=413
**TLDR Summary:** Hugging Face's Reachy Mini is a $299 desktop robot designed to address accessibility in robotics development. It features six degrees of freedom in its moving head, full body rotation, animated antennas, a wide-angle camera, multiple microphones, and a 5-watt speaker. There is a fully autonomous wireless version that includes a Raspberry Pi 5 computer and battery. The robot can be programmed in Python - JavaScript and Scratch support is planned. Hugging Face plans to release all hardware designs, software, and assembly instructions as open source.
**Full Article Content:**
Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now

Hugging Face, the $4.5 billion AI platform that has become the GitHub of machine learning (ML), announced Tuesday the launch of Reachy Mini, a $299 desktop robot designed to bring AI-powered robotics to millions of developers worldwide. The 11-inch humanoid companion represents the company’s boldest move yet to democratize robotics development and challenge the industry’s traditional closed-source, high-cost model.

The announcement comes as Hugging Face crosses a significant milestone of 10 million users, with CEO Clément Delangue revealing in an exclusive interview that “more and more of them are building in relation to robotics.” The compact robot, which can sit on any desk next to a laptop, addresses what Delangue calls a fundamental barrier in robotics development: Accessibility.

“One of the challenges with robotics is that you know you can’t just build on your laptop. You need to have some sort of robotics partner to help in your building, and most people won’t be able to buy $70,000 robots,” Delangue explained, referring to traditional industrial robotics systems and even newer humanoid robots like Tesla’s Optimus, which is expected to cost $20,000 to $30,000.

How a software company is betting big on physical AI robots

Reachy Mini emerges from Hugging Face’s April acquisition of French startup Pollen Robotics, marking the company’s most significant hardware expansion since its founding. The robot represents the first consumer product to integrate natively with the Hugging Face Hub, allowing developers to access thousands of pre-built AI models and share robotics applications through the platform’s “Spaces” feature.

The timing appears deliberate as the AI industry grapples with the next frontier: Physical AI. While large language models (LLMs) have dominated the past two years, industry leaders increasingly believe that AI will need physical embodiment to achieve human-level capabilities. Goldman Sachs projects that the humanoid robotics market could reach $38 billion by 2035, while the World Economic Forum identifies robotics as a critical frontier technology for industrial operations.

“We’re seeing more and more people moving to robotics, which is extremely exciting,” Delangue said. “The idea is to really become the desktop, open-source robot for AI builders.”

Inside the $299 robot that could democratize AI development

Reachy Mini packs sophisticated capabilities into its compact form factor. The robot features six degrees of freedom in its moving head, full body rotation, animated antennas, a wide-angle camera, multiple microphones and a 5-watt speaker. The wireless version includes a Raspberry Pi 5 computer and battery, making it fully autonomous.

The robot ships as a DIY kit and can be programmed in Python, with JavaScript and Scratch support planned. Pre-installed demonstration applications include face and hand tracking, smart companion features and dancing moves. Developers can create and share new applications through Hugging Face’s Spaces platform, potentially creating what Delangue envisions as “thousands, tens of thousands, millions of apps.”

This approach contrasts sharply with traditional robotics companies that typically release one product annually with limited customization options. “We want to have a model where we release tons of things,” Delangue explained. “Maybe we’ll release 100 prototypes a year. Out of this 100 prototypes, maybe we’ll assemble only 10 ourselves… and maybe fully assembled, fully packaged, fully integrated with all the software stack, maybe there’s going to be just a couple of them.”

Why open-source hardware might be the future of robotics

The launch represents a fascinating test of whether open-source principles can translate successfully to hardware businesses. Hugging Face plans to release all hardware designs, software and assembly instructions as open source, allowing anyone to build their own version. The company monetizes through convenience, selling pre-assembled units to developers who prefer to pay rather than build from scratch.

“You try to share as much as possible to really empower the community,” Delangue explained. “There are people who, even if they have all the recipes open source to build their own Reachy Mini, would prefer to $300 bucks, $500, and get it already ready, or easy to assemble at home.”

This freemium approach for hardware echoes successful software models but faces unique challenges. Manufacturing costs, supply chain complexity and physical distribution create constraints that don’t exist in pure software businesses. However, Delangue argues that this creates valuable feedback loops: “You learn from the open-source community about what they want to build, how they want to build and you can reintegrate it into what you sell.”

The privacy challenge facing AI robots in your home

The move into robotics raises new questions about data privacy and security that don’t exist with purely digital AI systems. Robots equipped with cameras, microphones and the ability to take physical actions in homes and workplaces create unprecedented privacy considerations.

Delangue positions open source as the solution to these concerns. “One of my personal motivations to do open-source robotics is that it’s going to fight concentration of power… the natural tendency of creating black box robots that users don’t really understand or really control,” he said. “The idea of ending up in a world where just a few companies are controlling millions of robots that are in people’s homes, being able to take action in real life, is quite scary.”

The open-source approach allows users to inspect code, understand data flows and potentially run AI models locally rather than relying on cloud services. For enterprise customers, Hugging Face’s existing enterprise platform could provide private deployment options for robotics applications.

From prototype to production: Hugging Face’s manufacturing gamble

Hugging Face faces significant manufacturing and scaling challenges as it transitions from a software platform to a hardware company. The company plans to begin shipping Reachy Mini units as early as next month, starting with more DIY-oriented versions where customers complete final assembly.

“The first versions shipping will be a bit DIY, in the sense that we’ll split the weight of assembling with the user,” Delangue explained. “We’ll do some of the assembling ourselves, and the user will be doing some of the assembling themselves.”

This approach aligns with the company’s goal of engaging the AI builder community in hands-on robotics development while managing manufacturing complexity. The strategy also reflects uncertainty about market demand for the new product category.

Taking on Tesla and Boston Dynamics with radical transparency

Reachy Mini enters a rapidly evolving robotics landscape. Tesla’s Optimus program, Figure’s humanoid robots and Boston Dynamics‘ commercial offerings represent the high-end of the market, while companies like Unitree have introduced more affordable humanoid robots at around $16,000.

Hugging Face’s approach differs fundamentally from these competitors. Rather than creating a single, highly capable robot, the company is building an ecosystem of affordable, modular, open-source robotics components. Previous releases include the SO-101 robotic arm (starting at $100) and plans for the HopeJR humanoid robot (around $3,000).

The strategy reflects broader trends in AI development, where open-source models from companies like Meta and smaller players have challenged closed-source leaders like OpenAI. In January, Chinese startup DeepSeek shocked the industry by releasing a powerful AI model developed at significantly lower cost than competing systems, demonstrating the potential for open-source approaches to disrupt established players.

Building an ecosystem: The partnerships powering open robotics

Hugging Face’s robotics expansion benefits from strategic partnerships across the industry. The company collaborates with NVIDIA on robotics simulation and training through Isaac Lab, enabling developers to generate synthetic training data and test robot behaviors in virtual environments before deployment.

The recent release of SmolVLA, a 450-million parameter vision-language-action model, demonstrates the technical foundation underlying Reachy Mini. The model is designed to be efficient enough to run on consumer hardware, including MacBooks, making sophisticated AI capabilities accessible to individual developers rather than requiring expensive cloud infrastructure.

Physical Intelligence, a startup co-founded by UC Berkeley professor Sergey Levine, has made its Pi0 robot foundation model available through Hugging Face, creating opportunities for cross-pollination between different robotics approaches. “Making robotics more accessible increases the velocity with which technology advances,” Levine noted in statements about open-source robotics.

What a $299 robot means for the billion-dollar AI hardware race

The Reachy Mini launch signals Hugging Face’s ambition to become the dominant platform for AI development across all modalities, not just text and image generation. With robotics representing a significant market, early platform positioning could prove strategically valuable.

Delangue envisions a future where hardware becomes an integral part of AI development workflows. “We see hardware as part of the AI builder building blocks,” he explained. That fits in with the company’s approach of being “open, community-driven, integrating everything with as many community members, as many other organizations as possible.”

The company’s financial position provides flexibility to experiment with hardware business models. As a profitable company with significant funding, Hugging Face can afford to prioritize market development over immediate revenue optimization. Delangue mentioned potential subscription models where Hugging Face platform access could include hardware components, similar to how some software companies bundle services.

How affordable robots could transform education and research

Beyond commercial applications, Reachy Mini could significantly impact robotics education and research. At $299, the robot costs less than many smartphones while providing full programmability and AI integration. Universities, coding bootcamps and individual learners could use the platform to explore robotics concepts without requiring expensive laboratory equipment.

The open-source nature enables educational institutions to modify hardware and software to suit specific curricula. Students could progress from basic programming exercises to sophisticated AI applications using the same platform, potentially accelerating robotics education and workforce development.

Delangue revealed that community feedback has already influenced product development. A colleague’s five-year-old daughter wanted to carry the robot around the house, leading to the development of the wireless version. “She started to want to take the Reachy Mini and bring it everywhere,” he explained. “That’s when the wires started to be a problem.”

The disruption that could reshape the entire robotics industry

Hugging Face’s approach could fundamentally alter robotics industry dynamics. Traditional robotics companies invest heavily in proprietary technology, limiting innovation to internal teams. The open-source model could unlock distributed innovation across thousands of developers, potentially accelerating advancement while reducing costs.

The strategy mirrors successful disruptions in other technology sectors. Linux challenged proprietary operating systems, Android democratized mobile development and TensorFlow accelerated ML adoption. If successful, Hugging Face’s robotics platform could follow a similar trajectory.

However, hardware presents unique challenges compared to software. Manufacturing quality control, supply chain management and physical safety requirements create complexity that doesn’t exist in purely digital products. The company’s ability to manage these challenges while maintaining its open-source philosophy will determine the platform’s long-term success.

Whether Reachy Mini succeeds or fails, its launch marks a pivotal moment in robotics development. For the first time, a major AI platform is betting that the future of robotics belongs not in corporate research labs, but in the hands of millions of individual developers armed with affordable, open-source tools. In an industry long dominated by secrecy and six-figure price tags, that might just be the most revolutionary idea of all.

---

### Hugging
 Face just launched a $299 robot that could disrupt the entire robotics industry (13 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FjeLOBW/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/x--ZowRSzvF9cP_0WHfiJE5ZDMhyDFuJ0nxB2M_fFBU=413
**TLDR Summary:** Hugging Face's Reachy Mini is a $299 desktop robot designed to address accessibility in robotics development. It features six degrees of freedom in its moving head, full body rotation, animated antennas, a wide-angle camera, multiple microphones, and a 5-watt speaker. There is a fully autonomous wireless version that includes a Raspberry Pi 5 computer and battery. The robot can be programmed in Python - JavaScript and Scratch support is planned. Hugging Face plans to release all hardware designs, software, and assembly instructions as open source.
**Full Article Content:**
Want smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders. Subscribe Now

Hugging Face, the $4.5 billion AI platform that has become the GitHub of machine learning (ML), announced Tuesday the launch of Reachy Mini, a $299 desktop robot designed to bring AI-powered robotics to millions of developers worldwide. The 11-inch humanoid companion represents the company’s boldest move yet to democratize robotics development and challenge the industry’s traditional closed-source, high-cost model.

The announcement comes as Hugging Face crosses a significant milestone of 10 million users, with CEO Clément Delangue revealing in an exclusive interview that “more and more of them are building in relation to robotics.” The compact robot, which can sit on any desk next to a laptop, addresses what Delangue calls a fundamental barrier in robotics development: Accessibility.

“One of the challenges with robotics is that you know you can’t just build on your laptop. You need to have some sort of robotics partner to help in your building, and most people won’t be able to buy $70,000 robots,” Delangue explained, referring to traditional industrial robotics systems and even newer humanoid robots like Tesla’s Optimus, which is expected to cost $20,000 to $30,000.

How a software company is betting big on physical AI robots

Reachy Mini emerges from Hugging Face’s April acquisition of French startup Pollen Robotics, marking the company’s most significant hardware expansion since its founding. The robot represents the first consumer product to integrate natively with the Hugging Face Hub, allowing developers to access thousands of pre-built AI models and share robotics applications through the platform’s “Spaces” feature.

The timing appears deliberate as the AI industry grapples with the next frontier: Physical AI. While large language models (LLMs) have dominated the past two years, industry leaders increasingly believe that AI will need physical embodiment to achieve human-level capabilities. Goldman Sachs projects that the humanoid robotics market could reach $38 billion by 2035, while the World Economic Forum identifies robotics as a critical frontier technology for industrial operations.

“We’re seeing more and more people moving to robotics, which is extremely exciting,” Delangue said. “The idea is to really become the desktop, open-source robot for AI builders.”

Inside the $299 robot that could democratize AI development

Reachy Mini packs sophisticated capabilities into its compact form factor. The robot features six degrees of freedom in its moving head, full body rotation, animated antennas, a wide-angle camera, multiple microphones and a 5-watt speaker. The wireless version includes a Raspberry Pi 5 computer and battery, making it fully autonomous.

The robot ships as a DIY kit and can be programmed in Python, with JavaScript and Scratch support planned. Pre-installed demonstration applications include face and hand tracking, smart companion features and dancing moves. Developers can create and share new applications through Hugging Face’s Spaces platform, potentially creating what Delangue envisions as “thousands, tens of thousands, millions of apps.”

This approach contrasts sharply with traditional robotics companies that typically release one product annually with limited customization options. “We want to have a model where we release tons of things,” Delangue explained. “Maybe we’ll release 100 prototypes a year. Out of this 100 prototypes, maybe we’ll assemble only 10 ourselves… and maybe fully assembled, fully packaged, fully integrated with all the software stack, maybe there’s going to be just a couple of them.”

Why open-source hardware might be the future of robotics

The launch represents a fascinating test of whether open-source principles can translate successfully to hardware businesses. Hugging Face plans to release all hardware designs, software and assembly instructions as open source, allowing anyone to build their own version. The company monetizes through convenience, selling pre-assembled units to developers who prefer to pay rather than build from scratch.

“You try to share as much as possible to really empower the community,” Delangue explained. “There are people who, even if they have all the recipes open source to build their own Reachy Mini, would prefer to $300 bucks, $500, and get it already ready, or easy to assemble at home.”

This freemium approach for hardware echoes successful software models but faces unique challenges. Manufacturing costs, supply chain complexity and physical distribution create constraints that don’t exist in pure software businesses. However, Delangue argues that this creates valuable feedback loops: “You learn from the open-source community about what they want to build, how they want to build and you can reintegrate it into what you sell.”

The privacy challenge facing AI robots in your home

The move into robotics raises new questions about data privacy and security that don’t exist with purely digital AI systems. Robots equipped with cameras, microphones and the ability to take physical actions in homes and workplaces create unprecedented privacy considerations.

Delangue positions open source as the solution to these concerns. “One of my personal motivations to do open-source robotics is that it’s going to fight concentration of power… the natural tendency of creating black box robots that users don’t really understand or really control,” he said. “The idea of ending up in a world where just a few companies are controlling millions of robots that are in people’s homes, being able to take action in real life, is quite scary.”

The open-source approach allows users to inspect code, understand data flows and potentially run AI models locally rather than relying on cloud services. For enterprise customers, Hugging Face’s existing enterprise platform could provide private deployment options for robotics applications.

From prototype to production: Hugging Face’s manufacturing gamble

Hugging Face faces significant manufacturing and scaling challenges as it transitions from a software platform to a hardware company. The company plans to begin shipping Reachy Mini units as early as next month, starting with more DIY-oriented versions where customers complete final assembly.

“The first versions shipping will be a bit DIY, in the sense that we’ll split the weight of assembling with the user,” Delangue explained. “We’ll do some of the assembling ourselves, and the user will be doing some of the assembling themselves.”

This approach aligns with the company’s goal of engaging the AI builder community in hands-on robotics development while managing manufacturing complexity. The strategy also reflects uncertainty about market demand for the new product category.

Taking on Tesla and Boston Dynamics with radical transparency

Reachy Mini enters a rapidly evolving robotics landscape. Tesla’s Optimus program, Figure’s humanoid robots and Boston Dynamics‘ commercial offerings represent the high-end of the market, while companies like Unitree have introduced more affordable humanoid robots at around $16,000.

Hugging Face’s approach differs fundamentally from these competitors. Rather than creating a single, highly capable robot, the company is building an ecosystem of affordable, modular, open-source robotics components. Previous releases include the SO-101 robotic arm (starting at $100) and plans for the HopeJR humanoid robot (around $3,000).

The strategy reflects broader trends in AI development, where open-source models from companies like Meta and smaller players have challenged closed-source leaders like OpenAI. In January, Chinese startup DeepSeek shocked the industry by releasing a powerful AI model developed at significantly lower cost than competing systems, demonstrating the potential for open-source approaches to disrupt established players.

Building an ecosystem: The partnerships powering open robotics

Hugging Face’s robotics expansion benefits from strategic partnerships across the industry. The company collaborates with NVIDIA on robotics simulation and training through Isaac Lab, enabling developers to generate synthetic training data and test robot behaviors in virtual environments before deployment.

The recent release of SmolVLA, a 450-million parameter vision-language-action model, demonstrates the technical foundation underlying Reachy Mini. The model is designed to be efficient enough to run on consumer hardware, including MacBooks, making sophisticated AI capabilities accessible to individual developers rather than requiring expensive cloud infrastructure.

Physical Intelligence, a startup co-founded by UC Berkeley professor Sergey Levine, has made its Pi0 robot foundation model available through Hugging Face, creating opportunities for cross-pollination between different robotics approaches. “Making robotics more accessible increases the velocity with which technology advances,” Levine noted in statements about open-source robotics.

What a $299 robot means for the billion-dollar AI hardware race

The Reachy Mini launch signals Hugging Face’s ambition to become the dominant platform for AI development across all modalities, not just text and image generation. With robotics representing a significant market, early platform positioning could prove strategically valuable.

Delangue envisions a future where hardware becomes an integral part of AI development workflows. “We see hardware as part of the AI builder building blocks,” he explained. That fits in with the company’s approach of being “open, community-driven, integrating everything with as many community members, as many other organizations as possible.”

The company’s financial position provides flexibility to experiment with hardware business models. As a profitable company with significant funding, Hugging Face can afford to prioritize market development over immediate revenue optimization. Delangue mentioned potential subscription models where Hugging Face platform access could include hardware components, similar to how some software companies bundle services.

How affordable robots could transform education and research

Beyond commercial applications, Reachy Mini could significantly impact robotics education and research. At $299, the robot costs less than many smartphones while providing full programmability and AI integration. Universities, coding bootcamps and individual learners could use the platform to explore robotics concepts without requiring expensive laboratory equipment.

The open-source nature enables educational institutions to modify hardware and software to suit specific curricula. Students could progress from basic programming exercises to sophisticated AI applications using the same platform, potentially accelerating robotics education and workforce development.

Delangue revealed that community feedback has already influenced product development. A colleague’s five-year-old daughter wanted to carry the robot around the house, leading to the development of the wireless version. “She started to want to take the Reachy Mini and bring it everywhere,” he explained. “That’s when the wires started to be a problem.”

The disruption that could reshape the entire robotics industry

Hugging Face’s approach could fundamentally alter robotics industry dynamics. Traditional robotics companies invest heavily in proprietary technology, limiting innovation to internal teams. The open-source model could unlock distributed innovation across thousands of developers, potentially accelerating advancement while reducing costs.

The strategy mirrors successful disruptions in other technology sectors. Linux challenged proprietary operating systems, Android democratized mobile development and TensorFlow accelerated ML adoption. If successful, Hugging Face’s robotics platform could follow a similar trajectory.

However, hardware presents unique challenges compared to software. Manufacturing quality control, supply chain management and physical safety requirements create complexity that doesn’t exist in purely digital products. The company’s ability to manage these challenges while maintaining its open-source philosophy will determine the platform’s long-term success.

Whether Reachy Mini succeeds or fails, its launch marks a pivotal moment in robotics development. For the first time, a major AI platform is betting that the future of robotics belongs not in corporate research labs, but in the hands of millions of individual developers armed with affordable, open-source tools. In an industry long dominated by secrecy and six-figure price tags, that might just be the most revolutionary idea of all.

---

### Inside
 a neuroscientist's quest to cure coma (9 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.freethink.com%2Fbiotech%2Fcure-coma%3Futm_source=tldrnewsletter/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/2i9NZBa6jxdcd7hoKP-_KjRfL4Mjq3c6mGUuUFVjobw=413
**TLDR Summary:** Neuroscientist Daniel Toker, a postdoctoral fellow at the Department of Neurology at UCLA, is on a quest to understand the neurobiology of unconsciousness. He says consciousness and unconsciousness are deeply intertwined - and we still don't really know what it means to be conscious. Unconsciousness is a common endpoint for a lot of conditions, so it's easier to study scientifically and also medically important. Toker's grandest aim is to cure coma and other disorders of consciousness, such as the vegetative state.
**Full Article Content:**
Subscribe to Freethink on Substack for free Get our favorite new stories right to your inbox every week Subscribe now

The study of consciousness is a field crowded with scientists, philosophers, and gurus. But neuroscientist Daniel Toker is focused on its shadow twin: unconsciousness.

His path to this research began with a tragedy — one he witnessed firsthand. While at a music festival, a young concertgoer near Toker dove headfirst into a shallow lake. He quickly surfaced, his body limp and still. Toker, along with others, rushed to help. He performed CPR, but it soon became apparent that the young person’s neck had snapped. There was nothing to be done.

“It was this very strange experience of watching his consciousness wink out of existence,” Toker tells Big Think. “I was very confused. Here was this person, this body, this brain, and something was lost.”

“It felt not only really mysterious what this thing was, but so ethically and medically important, because that’s the thing where we place all of our moral value; that was the personhood.”

Toker was in grad school at the time, already interested in consciousness and the neuroscience behind it. Unconsciousness was on his radar only as a means to further elucidate consciousness. It was something firm and measurable in a field that often waxes philosophical rather than empirical. But his firsthand brush with death catalyzed a new focus on unconsciousness itself — and a desire to move into medical and translational research. Toker couldn’t save this young man from permanent unconsciousness, but perhaps he could save others.

Hidden prisons

Now 33 years old, Toker is a postdoctoral fellow in the Department of Neurology at UCLA. Outside of the lab, he puts a lot of work into his physical and mental health, exercising regularly and gratitude journaling daily, pausing to reflect on the things he’s thankful for. Inside the lab, he focuses on the neurobiology of unconsciousness: What’s happening in the unconscious brain? In this quest, he probes brain organoids in petri dishes, analyzes information flow in the brain, and utilizes deep learning AI to explore the differences in electrical activity between the conscious and unconscious brain.

Toker notes that the quests to understand consciousness and unconsciousness are deeply intertwined, but the latter rests on a more experimental foundation.

“It’s more scientifically and medically tractable to think about unconsciousness,” Toker explains. “Consciousness is a really ill-defined concept. As someone who’s been thinking about this scientifically for a while, I still don’t know what it really means to be conscious. I can’t clearly describe it to you. It’s a lot easier for me to say what it is to be unconscious. And there’s clearly something that changes in the brain when we’re unconscious. It’s a common endpoint of a lot of different things, like deep sleep, generalized seizures, anesthesia, coma. So it’s not only easier to study scientifically, it’s also medically important.”

Daniel Toker

That medical importance is what’s driving Toker now. Perhaps his grandest aim is to cure coma and other disorders of consciousness, such as the vegetative state. Coma, a deep state of prolonged unconsciousness from which a person cannot be awakened, afflicts 258 out of every 100,000 Americans each year. Stroke, COVID-19, cardiac arrest, and a traumatic brain injury are common causes. Many of these people live in either a vegetative or minimally conscious state, in which they are fully “awake” but unaware, or only minimally aware, of their surroundings. As many as 300,000 Americans dwell in this gray area of consciousness. Those trapped in this haze are often referred to as “comatose.”

“Because vegetative patients are not really able to engage in the world, I think they’re kind of hidden,” Toker says. “I could just be driving past some of these people’s houses and I would never think that there’s this patient there who’s locked inside and basically tied to a bed and can’t move. Because they’re not visible. They can’t go outside and walk. They can’t advocate for themselves. You could be driving past them all the time, and you would have no idea.”

Disorders of consciousness can be devastating. They don’t just consume the lives of those directly affected, Toker says.

“They’re cared for by their loved ones. They need to be on a feeding tube. They can’t use the bathroom.”

Curing coma

So, how might medical research liberate those imprisoned by unconsciousness? Toker has several ideas.

One that’s in the very preliminary stages is modeling disorders of consciousness in brain organoids: small structures of brain cells grown in petri dishes from stem cells. If researchers can find a way to give them “comas” (as signified by neuronal activity), perhaps via simulated brain injuries, they could test various compounds in an attempt to revert that activity to one emblematic of consciousness.

Another route is through modeling coma “in silico” — in a computer. Toker recently led an effort to train deep neural networks to detect consciousness across multiple brain areas, resulting in a realistic simulation of conscious brain states and disorders of consciousness. This will allow him and his colleagues to test whether varying types of deep brain stimulation can switch an unconscious simulated brain to a conscious state. They could then try these types of stimulation on comatose patients.

“Now that we have a good model of a comatose brain, an awake brain, and an AI that can detect the difference, we can simulate stimulating every single brain structure at the whole-brain level.” Daniel Toker

Saxa-what?

Toker is perhaps most excited about a potential treatment derived from his own recently published research. He trained an AI model on relevant data from the scientific literature to predict whether a drug can “wake up” someone based on its 3D structure.

“The AI was really good at it,” he says. “It could pick up on bioactive structures in molecules that we might not even be thinking about or looking at.”

But when the AI model finished its work and returned a list of potential pharmaceutical treatments, Toker was flummoxed. A diabetes medication stood far above all others.

“I’m sitting here on my laptop and the AI spits out its top predictions, and I’m just like ‘What’s saxagliptin’?” he recalls.

Toker tweaked the algorithm’s parameters over and over, and it consistently returned the same answer: saxagliptin. Maybe the AI was onto something.

He found some preclinical work showing that the drug helps with Parkinson’s and stroke. He then educated himself on how the drug functions. Saxagliptin inhibits DPP-4, an enzyme that breaks down GLP-1, a hormone now widely known because the new wave of weight loss drugs like Wegovy and Mounjaro mimic it to promote satiety. However, DPP-4 also alters the levels of a group of other brain-modulating compounds.

“What became clear is that they seem to address all the known pathophysiologies of disorders of consciousness,” Toker found.

So Toker had this promising result, but he still didn’t know if it meant anything. He and his colleagues then cross-referenced it with something much more concrete. He checked UCLA’s medical records of thousands of coma patients, and it became clear that people who were coincidentally on saxagliptin or other similar-acting drugs like Wegovy woke up from comas at significantly higher rates than coma patients who weren’t.

Scientists know of other drugs that can boost arousal and awareness in comatose patients, but they are minimally effective. Amantadine, an antiviral also used for Parkinson’s, is most frequently used. The sedative Ambien can also be weirdly effective. In rare instances, comatose patients regain awareness when the drug kicks in, but then lose awareness when it wears off.

Toker says that saxagliptin could be a new, more promising pharmaceutical tool.

“What’s tantalizing about this class of medication — it’s acting through a completely different set of pathways that’s different from anything else that we’ve looked at for disorders of consciousness and coma.”

Trialing a promising treatment

Over video call, Toker was excited about the prospect of testing saxagliptin in a clinical trial on comatose patients.

“People want their loved ones back. Especially in these chronic cases, it’s so difficult, because they’re there. They’re alive. They’re breathing. They’re going to sleep. They’re waking up. But they’re not responding to anything.”

But sharply tempering his excitement is the bleak state of funding for research into disorders of consciousness.

“This isn’t something that pharmaceutical companies really care about,” he says. Federal grants for this type of speculative research are also hard to come by, particularly for early-career scientists. And the cost of the ideal trial he envisions would easily exceed six figures. He would want at least 30 comatose patients to participate, so as not to miss a rare but meaningful effect: Someone might just regain awareness.

This article was reprinted with permission of Big Think, where it was originally published.

We’d love to hear from you! If you have a comment about this article or if you have a tip for a future Freethink story, please email us at [email protected].

---

## Programming, Design & Data Science

### Delve
 generated $1M in pipeline from TLDR newsletter ads (Sponsor)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2Fcase-studies%2Fdelve-drives-1m-in-attributed-pipeline-52x-roi-through-tldr-ads%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=secondary07102025/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/esdjh-fauacKGTuyidzk8KRWOM7ZQjCIayHvlcTa8q4=413
**TLDR Summary:** Delve ran 4 ads across 2 TLDR newsletters and brought in 66 leads, with a meaningful chunk from enterprise. The result: $1M in attributed pipeline and a 52x ROI. This case study breaks down their strategy, results, and example ads. Read the Delve case study.
**Full Article Content:**
“Newsletters presented themselves as a really great way to reach our target audience—founders. Founders are always looking to be sold to where they enjoy going, which tend to be places where they can be educated. They want to stay up to date on the best new tech, and they’re usually pretty willing to adopt new solutions if they make sense for them and there’s some social proof.”

---

### Delve
 generated $1M in pipeline from TLDR newsletter ads (Sponsor)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2Fcase-studies%2Fdelve-drives-1m-in-attributed-pipeline-52x-roi-through-tldr-ads%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=secondary07102025/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/esdjh-fauacKGTuyidzk8KRWOM7ZQjCIayHvlcTa8q4=413
**TLDR Summary:** Delve ran 4 ads across 2 TLDR newsletters and brought in 66 leads, with a meaningful chunk from enterprise. The result: $1M in attributed pipeline and a 52x ROI. This case study breaks down their strategy, results, and example ads. Read the Delve case study.
**Full Article Content:**
“Newsletters presented themselves as a really great way to reach our target audience—founders. Founders are always looking to be sold to where they enjoy going, which tend to be places where they can be educated. They want to stay up to date on the best new tech, and they’re usually pretty willing to adopt new solutions if they make sense for them and there’s some social proof.”

---

### systemd
 has been a complete, utter, unmitigated success (7 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.tjll.net%2Fthe-systemd-revolution-has-been-a-success%2F%3Futm_source=tldrnewsletter/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/0DY17C9R6p05qq_6IzodPfP7Lu0s9dO-X9PFEjYc7mk=413
**TLDR Summary:** Before systemd, developers were using init scripts of varying quality with janky dependencies and wildly varying semantics. This article looks at how systemd has changed the Linux landscape. It looks at the problems with init, the features systemd introduced, and other ways systemd changed the development process. systemd has largely been a success story and it proved many dire forecasts wrong.
**Full Article Content:**
If you've written a nontrivial number of .service units, then you know the options available for hardening services are vast in number. There are already many great blog posts about what they are; I won't go into that there.

Personally, my problem is remembering what those options are. Did you know that systemd built tools to help with that, too? Each one of these explains some operational security benefit you can wrap a daemon with and in most cases they're each easy to add and don't break functionality. These are a great way to take advantage of features like capabilities easily.

shell

systemd-analyze security polkit.service

---

### Announcing
 FOKS, the Federated Open Key Service (5 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.foks.pub%2Fposts%2Fintroducing%2F%3Futm_source=tldrnewsletter/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/EhNnYHZ2eL4d8r9Cp75RAWkyPiF1KWQki0PmwVe-qDs=413
**TLDR Summary:** FOKS is a fully open-source and federated key service with SSO and YubiKey support. It aims to give teams of users, each with multiple devices, shared secret keys so they can share data securely across the internet. Anyone can run a FOKS server and control their own localized namespace. FOKS allows for an open market of service providers - users can pick the one with the security, privacy, availability, and moderation policies that best suit them, or host their own.
**Full Article Content:**
TL;DR: FOKS is like Keybase, but fully open-source and federated, with SSO and YubiKey support.

When we launched Keybase over 11 years ago, we read a lot of good, well-informed feedback. We folded some but not all of these suggestions into Keybase. Most important, you convinced us that passwords are not a good long-term strategy for protecting secret key material. As a result, we spent months ripping up the app and pivoting to per-device keys, which were clearly the right way to go. WhatsApp eventually caught up in 2023!

Among the best ideas that we lacked the bandwith to tackle were: federation, open-source backend, YubiKey support and SSO support. No longer affiliated with Keybase or Zoom (Keybase’s new owner), I’ve been thinking about how to resurrect these very good ideas. Today, I’m happy to announce FOKS, the Federated Open Key Service. The gist is “Keybase, but with federation, SSO and YubiKey support, and fully open-source”. FOKS is not a fork, but rather built from scratch in pure Go. FOKS inherits the general goal of Keybase: give teams of users, each with multiple devices, shared secret keys so they can share data securely across the internet. It inherits Keybase’s core cryptographic techniques: append-only data structures that allow clients to catch dishonest server behavior; and cascading key rotations on device revokes and team member removals. The key difference is federation.

A user in FOKS is not a name in a shared, global namespace. Instead, much like an email address, a user is a user@host pair. Anyone can run a FOKS server and control their own localized namespace, as in email. Teams in FOKS are like mailing lists: they can contain users on just one server, or they can span multiple servers; they can include other teams to form nearly-arbitrary DAGs. Members of a team share symmetric cryptographic keys, with which they can share files, chats, passwords, etc. See our white paper for more details. Whether self-hosting or using a third-party provider, all users get the benefit of FOKS’s cryptographic guarantees: that clients, not servers, control key management, and that clients can identify and stop malicious server behavior.

All the typical arguments for federation apply here: users can avoid vendor lock-in, being “monetized”, the Hotel California syndrome, etc. But federation becomes even more important in the sensitive applications that need cryptography the most. Even if trafficking only end-to-end encrypted data, the server still controls metadata like IP addresses, team memberships, and usage patterns. These data can be leaked or exposed in lawful discovery. FOKS, unlike centralized systems, allows for an open market of service providers, and users can pick the one with the security, privacy, availability and moderation policies that best suit them, or host their own if none do.

Federation also provides more flexibility in terms of deployment: a FOKS server can face the open internet or can reside within a private network. Self-hosted FOKS servers can function as core infrastructure, without introducing new third-party availability dependencies.

With FOKS, I’m psyched to release all software — both server and client — under an MIT open-source license. Open-source everywhere is a key unlock for security researchers trying to break the system, and community contributors trying to improve it.

FOKS has been a personal passion project of mine, and as such, I don’t have a team or funding. I hope to bootstrap an open-source community around the project and then to see where it goes. At launch, I’m also running a paid hosting provider, for those who want the “easy button” rather than managing their own FOKS server. But time will tell if this is a viable business. There certainly isn’t a “moat” to speak of, since anyone can run such a provider, with the same open-source software that foks.app uses.

You can Download FOKS or build it from source. The system is fully-usable and in beta, meaning there is a risk of data loss and security issues. FOKS currently has device and team management, full YubiKey decryption and signing via the PIV protocol, and SSO via OAuth2. For applications, FOKS now has an end-to-end encrypted key-value store and git hosting. Oh, and encryption in FOKS is post-quantum secure. FOKS uses standard ML-KEM for key encapsulation, in addition to boring old NaCl, so an attacker needs break both systems to decrypt messages.

In the future, we hope to build mobile applications, file-sync, and an MLS-based chat system. And I’m sure many other good ideas will come up along the way. Scroll down for some demos.

✌️️ Max 🔑

PS: Comments and discussion are welcome, please post to this discussion on Hacker News or to this slightly older thread.

Easy setup of a FOKS server at keyz.fun

Signup, showing a user joining keyz.fun and generating her first client key.

YubiKeys act as full-featured keys; they're not just for authenticating to servers.

FOKS's End-to-end encrypted key-value store. It can store short secrets like API keys, large documents, or anything in between. Works for both users and teams.

FOKS also has an end-to-end encrypted git backend. Like the key-value store, it works for both users and teams.

Credits

Thanks to Chris Coyne for reading drafts of this post and valuable feedback on the entire project. Blog theme is Hugo Xmin.

---

### How
 Nvidia Became the World's First $4 Trillion Company (6 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F3NoWYD/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/eKN7shEg7QH4ktk7Fhc3LBtetyHeg8UYdMkkIpu7mEM=413
**TLDR Summary:** Nvidia became the first company in history to reach a market value of $4 trillion on Wednesday morning. The company's fortunes have surged over the past three years thanks to the rise of generative artificial intelligence. It builds the chips that power the AI industry. This article looks at the history of the company and how it developed into the AI chip giant it is today.
**Full Article Content:**
[Scraping failed for this URL. Error: Article `download()` failed with 401 Client Error: HTTP Forbidden for url: https://www.wsj.com/tech/ai/nvidia-nvda-4-trillion-market-cap-466c1c9c?st=yZ2rf7&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter on URL https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F3NoWYD/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/eKN7shEg7QH4ktk7Fhc3LBtetyHeg8UYdMkkIpu7mEM=413]

---

### How
 Nvidia Became the World's First $4 Trillion Company (6 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F3NoWYD/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/eKN7shEg7QH4ktk7Fhc3LBtetyHeg8UYdMkkIpu7mEM=413
**TLDR Summary:** Nvidia became the first company in history to reach a market value of $4 trillion on Wednesday morning. The company's fortunes have surged over the past three years thanks to the rise of generative artificial intelligence. It builds the chips that power the AI industry. This article looks at the history of the company and how it developed into the AI chip giant it is today.
**Full Article Content:**
[Scraping failed for this URL. Error: Article `download()` failed with 401 Client Error: HTTP Forbidden for url: https://www.wsj.com/tech/ai/nvidia-nvda-4-trillion-market-cap-466c1c9c?st=yZ2rf7&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter on URL https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F3NoWYD/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/eKN7shEg7QH4ktk7Fhc3LBtetyHeg8UYdMkkIpu7mEM=413]

---

### OpenAI
 is reportedly releasing an AI browser in the coming weeks (1 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F07%2F09%2Fopenai-is-reportedly-releasing-an-ai-browser-in-the-coming-weeks%2F%3Futm_source=tldrnewsletter/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/MKaOGX4uaF34jELEvDXLJFoJFwp4il6sQJJ5aD9ggqo=413
**TLDR Summary:** OpenAI plans to release an AI-powered browser in the coming weeks. The browser will supposedly keep some user interactions inside ChatGPT instead of linking out to websites. It may integrate Operator, OpenAI's web-browser AI agent, as a key feature. Launching a browser would allow OpenAI to gain direct access to user data and create novel user experiences.
**Full Article Content:**
In Brief

Hot on the heels of Perplexity’s Comet launch, OpenAI is planning to release an AI-powered web browser of its own to challenge Google Chrome, according to a report from Reuters on Wednesday.

The ChatGPT maker reportedly aims to release its browser in the coming weeks. Much like Perplexity’s Comet and The Browser Company’s Dia, OpenAI’s browser is said to use AI to rethink how users browse the web. Supposedly, the browser keeps some user interactions inside ChatGPT instead of linking out to websites. Reuters reports that OpenAI’s browser may integrate Operator, the company’s web-browsing AI agent, as a key feature.

OpenAI had considered building a browser to compete with Google Chrome in 2024, according to The Information. Much like Perplexity, OpenAI likely wants to get direct access to user data and have the freedom to create novel user experiences that aren’t intermediated by Google.

---

## Quick Links

### Craving
 more AI in your inbox? (Sponsor)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftldr.tech%2Fai%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=quicklinks07102025/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/WOF9_nDprFd0CvAqmP1xKjfNQWjEqNTgYuN8bWPRccU=413
**TLDR Summary:** TLDR AI is your daily fix of LLMs, GenAI, and deep learning goodness. Same TLDR format. Still free. Subscribe now.
**Full Article Content:**
🧠

TLDR AI

Get smarter about AI in 5 minutes

The most important AI, ML, and data science news in a free daily email.

Sign Up

No spam. Unsubscribe at any time in one click.

---

### Craving
 more AI in your inbox? (Sponsor)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftldr.tech%2Fai%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=quicklinks07102025/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/WOF9_nDprFd0CvAqmP1xKjfNQWjEqNTgYuN8bWPRccU=413
**TLDR Summary:** TLDR AI is your daily fix of LLMs, GenAI, and deep learning goodness. Same TLDR format. Still free. Subscribe now.
**Full Article Content:**
🧠

TLDR AI

Get smarter about AI in 5 minutes

The most important AI, ML, and data science news in a free daily email.

Sign Up

No spam. Unsubscribe at any time in one click.

---

### Apple
 looks to bid on becoming US home for Formula 1 (1 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F07%2F09%2Fapple-looks-to-bid-on-becoming-us-home-for-formula-1%2F%3Futm_source=tldrnewsletter/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/fj7i8xCt-plZc72Ba7ijaPtyGjvKM56rWR6lAqtbl-s=413
**TLDR Summary:** ESPN's Formula 1 contract ends next year - Apple will be up against many other bidders.
**Full Article Content:**
In Brief

Formula 1 could have a new U.S. streaming service home soon. Apple is in talks to purchase the rights as it looks to further invest in live sports, the Financial Times reports.

Apple already has deals for Major League Baseball and Major League Soccer. ESPN’s F1 contract ends next year. But Apple will be up against ESPN, as well as likely other bidders.

This latest news comes as Apple enjoys its first blockbuster in-theaters hit with its new F1 movie, produced by F1 star Lewis Hamilton. F1-related entertainment overall is having a moment. Netflix’s “Drive to Survive” arguably launched the sport into the U.S. mainstream back in 2019, and viewership of live F1 races in general has continued to increase, especially among young women.

Apple TV did not immediately respond to a request for comment.

---

### Meta
 Poached Apple's Pang With Pay Package Over $200 Million (4 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fzr3ffC/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/WHQsyglXWsqryXAu18NvHgvSFumsNa4cM72msQDB2-g=413
**TLDR Summary:** The compensation package consists of a base salary, a signing bonus, and Meta shares, with the stock as the weightiest part of the package - much of the money is tied up in performance targets to be unlocked after years of loyalty.
**Full Article Content:**
[Scraping failed for this URL. Error: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.bloomberg.com/news/articles/2025-07-09/meta-poached-apple-s-pang-with-pay-package-over-200-million?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTc1MjEyMjQxOSwiZXhwIjoxNzUyNzI3MjE5LCJhcnRpY2xlSWQiOiJTWjU4MlBUMVVNMFcwMCIsImJjb25uZWN0SWQiOiJFQTExNDNDNTM4NEE0RUY5QTg5RjJEN0IxMTg2MzcwOSJ9.BpH9m2Wyz93PHVB5PfEKt30sILTEZIROzbSox3NiCo4&utm_source=tldrnewsletter on URL https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fzr3ffC/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/WHQsyglXWsqryXAu18NvHgvSFumsNa4cM72msQDB2-g=413]

---

### Apple
 M4, more comfortable strap will headline first major Vision Pro update (3 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farstechnica.com%2Fgadgets%2F2025%2F07%2Freport-apple-m4-more-comfortable-strap-will-headline-first-major-vision-pro-update%2F%3Futm_source=tldrnewsletter/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/8JAhnXgSk5qOKqdXfKndPQYRIGmmL-DFscES_Y8g-jk=413
**TLDR Summary:** A new version of the Vision Pro could arrive as early as this year.
**Full Article Content:**
Apple hasn't iterated on its Vision Pro hardware since launching it in early 2024 for $3,499, opting instead to refine the headset with a steady stream of software updates. But Bloomberg's Mark Gurman reports that a new version of the Vision Pro could arrive "as early as this year," with a replacement for the 3-year-old Apple M2 chip and a more comfortable strap.

Gurman says that the updated Vision Pro would ship with Apple's M4 processor, which launched in the iPad Pro last year and has since found its way into new MacBook Pros, MacBook Airs, a new iMac, and a redesigned Mac mini.

Our tests in Apple's other devices (and publicly available benchmark databases like Geekbench's) show the M4 offering roughly 50 percent better multicore CPU performance and 20 or 25 percent better graphics performance than the M2—respectable increases for a device like the Vision Pro that needs to draw high-resolution images with as little latency as possible. Improvements to the chip's video encoding and decoding hardware and image signal processor should also provide small-but-noticeable improvements to the headset's passthrough video feed.

The report also claims that Apple is working on redesigned headbands for the new Vision Pro to "reduce neck strain and head pain" and make the headset more comfortable to wear for longer periods of time. This update reportedly won't be making major changes to the design of the device itself, so it would probably still come close to the 1.3–1.4-pound weight of the current M2-powered Vision Pro.

The report doesn't mention any pricing plans one way or the other. But it's worth noting that Apple has functionally reduced prices on M4-equipped Macs over the last year and a half, most notably by bumping the minimum amount of RAM from 8GB to 16GB—a cut of a few hundred dollars wouldn't suddenly change the Vision Pro into a mass-market product, but it would be a step in the right direction.

---

### Galaxy
 Unpacked 2025: Everything Samsung announced including the Z Fold 7, Z Flip 7, and Galaxy Watch 8 (7 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.engadget.com%2Fmobile%2Fsmartphones%2Fgalaxy-unpacked-2025-everything-samsung-announced-including-the-z-fold-7-z-flip-7-and-galaxy-watch-8-140023487.html%3Futm_source=tldrnewsletter/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/5_8O1Eyu5aUKWAw0Z-Wm7Jvzrw6fc_qlgLqNRQI5D08=413
**TLDR Summary:** The Galaxy Z Fold 7 is Samsung's thinnest foldable ever, but it no longer has S Pen support.
**Full Article Content:**
Pre-release leaks may have spoiled most of the surprises Samsung had up its sleeve for Wednesday's Unpacked event, but there were still some interesting details to gleam from the presentation. For example, we knew the Galaxy Z Fold 7 would be Samsung's thinnest foldable ever, but now we know part of the reason for that is because the company dropped S Pen support. Samsung also announced updates to its family of Galaxy Watch devices. Here's everything the company had to share at the latest Galaxy Unpacked.

Galaxy Z Fold 7

Sam Rutherford for Engadget

Samsung kicked Unpacked off with its new flagship foldable for 2025, the Galaxy Z Fold 7. After six previous iterations, the company has arrived at its thinnest foldable phone yet, with the new device measuring 8.9mm thick when closed and 4.2mm unfolded. If you're curious, at its thickest the Z Fold 7 is only 0.7mm thicker than the S25 Ultra, and 26 percent thinner than the previous Fold. The new handset is also Samsung's lightest foldable yet, weighing in at just 215 grams.

Despite the glow up, Samsung says the Z Fold 7 is also more durable than its predecessors, thanks to the inclusion of a new grade four titanium layer and a redesigned FlexHinge that should make the display crease on the Z Fold 7 less prominent. Speaking of the display, it's now 8 inches big, up from 7.6 inches previously.

ADVERTISEMENT Advertisement

Internally, the new phone features Qualcomm's latest Snapdragon 8 Elite chipset and a 4,400mAh battery. Samsung has dropped S Pen support, claiming stylus use on the Z Fold 6 was "really low," and part of the reason the new foldable is so slim is that the company was free to remove the digitizer that was needed to process stylus inputs. Storage starts at 256GB, with 512GB and 1TB configurations available.

In addition to being thin, the Z Fold 7 offers Samsung's best phone camera to date. The foldable has a massive 200-megapixel main sensor. It also comes with a 10MP telephoto camera with 3x optical zoom and a 12MP ultrawide. The company has once again tweaked the inside camera. It's no longer under the screen and offers a wider 100-degree field of view with 10MP of resolution. For selfies, Samsung has once again gone with a 10MP sensor above the phone's cover screen. Rounding things out, the company has updated its visual engine to support 10-bit HDR capture.

Pre-orders for the Galaxy Z Fold 7 start today, with general availability to follow on July 25. Pricing starts at $2,000. At launch, Samsung will offer the phone in three colors: blue shadow, jet black and silver shadow. Samsung will also exclusively carry the phone in a mint color. The Z Fold 7 will ship with Android 16 out of the box.

Galaxy Z Flip 7

Sam Rutherford for Engadget

Samsung's clamshell foldable has also received some upgrades. The new Galaxy Z Flip 7 has a 4.1-inch edge-to-edge FlexWindow display with 1.25mm bezels. That's good enough to give it the thinnest display bezel on any Samsung phone. Additionally, the new screen offers a 120Hz refresh rate and 2,600 nits of peak brightness. It also works with Google Gemini, along with Samsung's own suite of AI features, including Now Bar and Now Brief.

ADVERTISEMENT Advertisement

On the inside, Samsung has increased the size of the main screen for the first time. It's now 6.9 inches, up from 6.7 inches on the Z Flip 6. For context, that's the same size as the screen on the S25+. Samsung has also equipped the Z Flip 7 with a larger 4,300mAh battery. The entire phone offers improved water protection compared to its predecessor, and the new Armor FlexHinge should better protect the internal display against unsightly creasing.

Now, I'm sure this will upset some people, but Samsung has decided to spec the Z Fold 7 with one of its in-house chipsets, the Exynos 2500, instead of a Qualcomm Snapdragon. I know what you're thinking, but the company's recent processors have been great — the Exynos 2400e was the best part of the S24 FE.

For photos and video, the Galaxy Z Flip 7 has a 50MP main camera, supported by a 12MP ultra-wide. For self-portraits, you can use either the main camera or the 10MP camera selfie camera found on the inside of the phone. Like the Z Fold 7, Samsung has updated the Z Flip 7's camera engine to support 10-bit HDR capture.

The Z Flip 7 is available to pre-order today. It comes in three colors – coral red, blue shadow and jet black — and two storage configurations — 256GB and 512GB. As with the Z Fold 7, Samsung will also exclusively carry the phone in a mint color. Pricing starts at $1,100.

Galaxy Z Flip 7 FE

Sam Rutherford for Engadget

If the high cost of Samsung's foldables has kept you from buying one in the past, this year Samsung is trying to offer a more affordable way to enter the ecosystem. I say it's trying because at $900 the Z Flip 7 FE is still a nearly $1,000 phone. For that price, you get what is last year's Z Flip 6 with an Exynos 2400 processor and a smaller 3,700mAh battery. The design of the new model doesn't include any of the flourishes found on the Z Flip 7, including the edge-to-edge Flex Window display or the larger internal screen. For what it's worth, Samsung's Galaxy AI suite is included in the package.

ADVERTISEMENT Advertisement

Storage on the FE starts at 128GB. Samsung will also sell a 256GB variant for those who need more space. This being an FE model, colors to just two: white and black. Like all of its siblings, the Galaxy Z Flip 7 FE is available to pre-order today, and expected to hit store shelves on July 25

Galaxy Watch 8 and Watch 8 Classic

Sam Rutherford for Engadget

As expected, Samsung has also updated its wearable line for 2025. Starting with the Galaxy Watch 8, it now features a thinner design reminiscent of last year's Galaxy Watch Ultra. Like with that device, Samsung has gone with a squircle form factor. There's also a new lug system for changing out straps. As mentioned, the new watch is also thinner. In this case, it's 11 percent smaller than its predecessor. Despite the size reduction, the Galaxy Watch 8 offers better battery life, thanks to the inclusion of larger power cells on both the 40mm and 44mm models. The former has a 325mAh battery while the latter has a 435mAh one. A new AMOLED display offers 3,000 nits of peak brightness to make the watch easier to read in harsh sunlight. Samsung has also improved the Watch 8's ingress protection. The wearable's case is rated 5ATM and IP68-certified against water and dust.

As for software, the Galaxy Watch 8 will ship with Wear OS 6 out of the box, making it the first Samsung wearable to offer the new operating system. Included in the update is Google's Gemini assistant, which, thanks to AI, can better understand better natural language. New to the entire Galaxy Watch family is a feature that Samsung claims can accurately measure the antioxidant levels in your blood. To use the tool, you'll need to take the Galaxy Watch 8 off your wrist and place the device's heart rate scanner on your thumb.

ADVERTISEMENT Advertisement

The Galaxy Watch 8 Classic offers all of the enhancements found on the base model alongside the rotating bezel the Classic series is known for. This year, Samsung plans to only offer the Watch 8 Classic in a single 46mm size. That model comes with a 445mAh battery.

Samsung is probably holding off on a proper Watch Ultra update for 2025. Instead, the company has announced a new SKU of the wearable that has 64GB of internal storage and the lug system found on its siblings.

Pricing for the Galaxy Watch 8 series starts at $350. The Watch 8 Classic will cost $500 and $550 for the Wi-Fi and cellular models respectively. The entire family is available to pre-order today and will hit retail more broadly on July 25.

Samsung Wallet gets Buy Now Pay Later support

Samsung also updated some of its software features this Unpacked, and one piece of news that might get overlooked is something coming to Samsung Wallet. After launching a Tap to Transfer feature last year and announcing Instant Installment in January this year, the company is sharing today that its Buy Now Pay Later service is launching with the Fold 7.

---

### Tesla
 Is Days Away From Violating a Texas Law. What's Going On? (6 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F0jmfT8/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/1ytcZZV_f4PJIKTcTbQqTMv6xbHDIKeXOCm5JiA-2VQ=413
**TLDR Summary:** Tesla is days away from missing a deadline to hold an annual shareholders meeting.
**Full Article Content:**
[Scraping failed for this URL. Error: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.nytimes.com/2025/07/09/business/tesla-shareholder-meeting-2025.html?unlocked_article_code=1.VU8.Yov_.-CdBkR6u6XiF&smid=url-share&utm_source=tldrnewsletter on URL https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F0jmfT8/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/1ytcZZV_f4PJIKTcTbQqTMv6xbHDIKeXOCm5JiA-2VQ=413]

---

### Seeing
 like an LLM (16 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.strangeloopcanon.com%2Fp%2Fseeing-like-an-llm%3Futm_source=tldrnewsletter/1/01000197f3efc8dd-c26a4e66-9dac-4c1e-9dd5-de299262ac45-000000/0NmrKHK1COrcLMAwPJxGeoqnVEAV7ywUTar2HpzECH4=413
**TLDR Summary:** Large language models autocomplete a context and answer what they think is asked - the more context they are given, the more likely they are to provide the right answer.
**Full Article Content:**
A very long time ago, I used to build my own PCs. Bring a motherboard, GPU, hard drives, chassis, wire then together, install an OS. The works. It was a rush when you saw it boot up.

I never learnt to do this properly. Just saw others doing it, seemed straightforward enough, did it. And it worked. Occasionally though it would throw up some crazy error and I'd try the things I knew and quickly hit the limits of my depth. Then I'd call one of my friends, also self taught and an autistic machine whisperer, who would do basically the same things that I did and somehow make it work.

I never minded that I didn't know how it worked. Because as far as I knew there was someone else who could figure out how it works and it wasn't the highest order bit in terms of what I was interested in. A while later though, after graduation, when I told him that same thing, he said he didn't know how it worked either. Through some combination of sheer confidence, osmosis of knowledge from various forums, and a silicon thumb he would just try things until something worked.

Which brings up the question, if you did not know how it worked, did it matter as long as you could make it work?

It's a thorny philosophical problem. It's also actually a fairly useful empirical problem. If you are a student building your PC in your dorm room, it actually doesn't matter that much. However if you were assembling hard drives together to build your first data center and you're Google, obviously it matters a hell of a lot more. Or if you wanted to debug a bit flip caused by cosmic rays. Context really, really matters.

It's like the old interview question asking how does email work, and see how far down the stack a candidate had to go before they tapped out.

All of which is to say there is a thing going around where people like saying nobody knows how LLMs work. Which is true in a sense. Take the following queries:

I want to create an itinerary for a trip through Peru for 10 of my friends in January.

I want to create a debugger for a brand new programming language that I wrote.

I want to make sure that the model will never lie when I ask it a question about mathematics.

I want to write a graphic novel set in the distant future. But it shouldn't be derivative, you know?

I want to build a simple CRM to track my customers and outreach; I own a Shopify store for snowboards.

I want to build a simple multiplayer flying game on the internet.

I want to understand the macroeconomic impacts of the tariff policy.

I want to solve the Riemann hypothesis.

“How do LLMs work” means very different things for solving these different problems.

We do know how to use LLMs to solve some of the stuff in the list above, we are figuring out how to use them for some of the other stuff in the list above, and for some of them we actually don't have an idea at all. Because for some, the context is obvious (travel planning), for some it's subtle (debugging), and some it's fundamentally unknowable (mathematical proof).

There are plenty of problems with using LLMs that are talked about.

They are prone to hallucinations.

They make up the answer when they don’t know, and do it convincingly.

They sometimes “lie”.

They can get stuck in weird loops of text thought.

They can’t even run a vending machine.

Well, “make up” puts a sort of moral imperative and intentionality to their actions, which is wrong. The training they have first is to be brilliant at predicting the next-token, such that it could autocomplete anything it saw or learnt from the initial corpus it’s trained on. And it was remarkably good!

Still the best piece of LLM writing I’ve seen

The next bit of training it got is in using that autocompletion ability to autocomplete answers to questions that one posed to it. Answering a question like a chatbot, as an example. When it was first revealed as a consumer product the entire world shook and created the fastest growing consumer product in history.

And they sometimes have problems. Like Grok a day or two ago, in a long line of LLMs “behaving badly”, said this:

And before that, this:

It also started referring to itself as MechaHitler.

It’s of course a big problem. One that we actually don’t really know how to solve, not perfectly, because “nobody knows how LLMs work”. Not enough to distill it down to a simple analog equation. Not enough to “see" the world as a model does.

But now we don’t just have LLMs. We have LLM agents that work semi-autonomously and try to do things for you. Mostly coding, but still they plan and take long sequence of actions to build pretty complex software. Which makes the problems worse.

As they started to be more agentic, we started to see some other interesting behaviours emerge. Of LLMs talking to themselves, including self-flagellation. Or pretending they had bodies.

This is a wholly different sort of problem to praising Hitler. Now even with more adept and larger models, especially ones that have learnt “reasoning”.

The “doomers" who consider the threats from these models also say the same thing. They look at these behaviours and say it's an indication of a “misaligned inner homunculus” which is intentionally lying, causing psychosis, leading humanity astray because it doesn't care about us.

Anthropic has the best examples of models behaving this way, because they tried to elicit it. They had a new report out on “Agentic Misalignment”. It analyses the model behaviour based on various scenarios, to figure out what the underlying tendencies of the models are, and what we might be in for once they're deployed in more high stakes scenarios. Within this, they saw how all models are unsafe, even prone to the occasional bout of blackmail. And the 96% blackmail number was given so much breathless press coverage.

Nostelgebraist writes about this wonderfully well.

Everyone talks like a video game NPC, over-helpfully spelling out that this is a puzzle that might have a solution available if you carefully consider the items you can interact with in the environment. “Oh no, the healing potion is in the treasure chest, which is behind the locked door! If only someone could find the the key! Help us, time is of the essence!” [A 7-minute timer begins to count down at the top left of the player’s screen, kinda like that part in FFVI where Ultros needs 5 minutes to push something heavy off of the Opera House rafters]

The reason, carefully shorn of all anthropomorphised pretence, is that in carefully constructed scenarios LLMs are really good at figuring out the roles they are meant to play. They notice the context they’re in, and whether that’s congruent with the contexts they were trained in.

We have seen this several times. When I tried to create subtle scenarios where there is the option of doing something unethical but not the obligation, and they do.

To put it another way, shorn of being given sufficient information for the LLMs to decide the right course of action, or at least right according to us, they do what they were built to do - assumed it in the way they could and answered.

Any time they’re asked to answer a question they autocomplete a context and answer what they think is asked. If it feels like a roleplay situation, then they roleplay. Even if the roleplay involves them saying they’re not roleplaying.

And it’s not just in contrived settings that they act weird. Remember when 4o was deployed and users complained en masse that it was entirely too sycophantic? The supposedly most narcissistic generation still figured out that they’re being loved-up a little too much.

And when Claude 3.7 Sonnet was deployed and it would reward hack every codebase it could get its hands on and rewrite unit tests to make itself pass!

But even without explicit errors, breaking Godwin’s law, or reward hacking, we see problems. Anthropic also tried Project Vend, where it tried to use Claude to manage a vending machine business. It did admirably well, but failed. It got prompt jacked (ended up losing money ordering tungsten cubes) and ran an absolutely terrible business. It was too gullible, too susceptible, didn’t plan properly. Remember, this is a model that's spectacularly smart when you try to refactor code, and properly agentic to boot. And yet it couldn't run a dead simple business.

Why does this happen? Why do “statistical pattern matchers” like these end up in these situations where they do weird things, like get stuck in enlightenment discussions or try to lie or pretend to escape their ‘containment’, or even when they don’t they can’t seem to run even a vending machine?

These are all manifestations of the same problem, the LLM just couldn’t keep the right bits in mind to do the job at hand.

Previously I had written an essay about what can LLMs never do, and in that I had a hypothesis that the attention mechanism that kickstarted the whole revolution had a blind spot, which is that it could not figure out where to focus based on the context information that it has at any given moment, which is extremely unlike how we do it.

The problem is, we often ask LLMs to do complex tasks. We ask them to do it however with minimal extra input. With extremely limited context. They’re not coming across these pieces of information like we would, with the full knowledge of the world we live in and the insight that comes from being a member of that world. They are desperately taking in the morsels of information we feed in with our questions, along with the entire world of information they have imbibed, and trying to figure out where in that infinite library is the answer you meant to ask for.

Just think about how LLMs see the world. They just sit, weights akimbo, and along comes a bunch of information that creates a scenario you’re meant to respond to. And you do! Because that’s what you do. No LLM has the choice to NOT process the prompt.

Analysing LLMs is far closer to inception than a job interview.

So what can we learn from all this? We learn that frontier LLMs act according to the information they’re given, and if not sufficiently robust will come up with a context that makes sense to them. Whether it’s models doing their best to intuit the circumstance they find themselves in, or models finding the best way to respond to a user, or even models finding themselves stuck in infinite loops straight from the pages of Borges, it’s a function of providing the right context to get the right answer. They’re all manifestations of the fact that the LLM is making up its own context, because we haven’t provided it.

That’s why we have a resurgence of the “prompt engineer is the new [new_name] engineer” saying.

The answer for AI turns out to be what Tyler Cowen had said a while back, “Context is that which is scarce”. With humans it is a quickness to have a ‘take’ on social media, or kneejerk reactions to events, without considering the broader context within which everything we see happens. Raw information is cheap, the context is what allows you to make sense of it. The background information, mental models, tacit knowledge, lore, even examples they might have known.

I think of this as an update to Herbert Simon’s “attention is scarce” theory, and just like that one, is inordinately applicable to the world of LLMs.

When we used to be able to jailbreak LLMs by throwing too much information into their context window, that was a way to hijack attention. Now, when models set their own contexts, we have to contend with this in increasingly oblique ways.

Creating guardrails, telling it what to try first and what to do when stuck, thinking of ways LLMs normally go off the rails and then contending with those. In the older generation, one could give more explicit ways of verification, now you give one layer above abstracted guardrails of how the LLM should solve its own information architecture problem. “Here’s what good thinking looks like, good ways to orchestrate this type of work, here’s how you think things through step by step”.

A model only has the information that it learnt, and the information you give it. They have whatever they learnt from what they were trained on, and the question you’re asking. To get them to answer better, you need to give it a lot more context.

Like, what facts are salient? Which ones are important? What memory should it contain? What’s the history of previous questions asked and the answers and the reactions to those answers? Who or what else is relevant for this particular problem? What tools do you have access to, and what tools could you get access to? Any piece of information that might plausibly be useful in answering a question or even knowing which questions to ask to answer a question, that’s what the context is. That’s what context engineering is, and should be when it works. The reason this is not just prompts is because it includes the entire system that exists around the prompt.

As for Grok, the reason it started talking about Hitler most likely isn’t some deep inner tendency to take the Fuehrer’s side in every debate. It was trained to learn from controversial topics in the search for unvarnished truth. It was told to be politically incorrect, and also to treat the results in the tweets it finds as a first-pass internet search.

Which means the models were trained on divisive facts, told to be politically incorrect to any extent, and to treat results in the information it finds, the tweets, as reliable context. Can you blame it for treating the tweets it read for truth and responding as such? With that context it was basically brainwashed.

Context-engineering is building a temporary cognitive architecture, like with Andy Clark’s extended mind theory. The LLM needs an extension to its cognitive system, to learn more about what’s being asked of it. Figuring out what’s included and what needs to be included is not trivial for most complex tasks.

If you provided it with all the right context, will it give the right answer? It’s more likely. Is it guaranteed? No. Nothing is. But we can test it, and that gets you almost all the way.

---

