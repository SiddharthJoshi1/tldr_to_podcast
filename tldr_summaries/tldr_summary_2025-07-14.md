# TLDR Newsletter Summary: 2025-07-14

## Big Tech & Startups

### Google
 hires Windsurf CEO Varun Mohan, others in $2.4 billion AI talent deal (3 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cnbc.com%2F2025%2F07%2F11%2Fgoogle-windsurf-ceo-varun-mohan-latest-ai-talent-deal-.html%3Futm_source=tldrnewsletter/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/e7OE18VbvzW2NrPJrEl6DcCO7GayS13e9xG6Y5uVY90=413
**TLDR Summary:** Google has signed an agreement to hire Varum Mohan, the co-founder of AI coding startup, Windsurf, and other senior Windsurf research and development employees. Google will not invest in Windsurf, but it will receive a nonexclusive license to certain Windsurf technology. Windsurf is free to license its technology to others. Google is paying $2.4 billion for the deal. Windsurf had been in talks with OpenAI for a $3 billion acquisition deal, but the finalization period expired.
**Full Article Content:**
Google on Friday made the latest splash in the AI talent wars, announcing an agreement to bring in Varun Mohan, co-founder and CEO of artificial intelligence coding startup Windsurf.

As part of the deal, Google will also hire other senior Windsurf research and development employees. Google is not investing in Windsurf, but the search giant will take a nonexclusive license to certain Windsurf technology, according to a person familiar with the matter. Windsurf remains free to license its technology to others.

The person familiar with the deal says Google is paying $2.4 billion in licensing fees and for compensation.

"We're excited to welcome some top AI coding talent from Windsurf's team to Google DeepMind to advance our work in agentic coding," a Google spokesperson wrote in an email. "We're excited to continue bringing the benefits of Gemini to software developers everywhere."

The deal between Google and Windsurf comes after the AI coding startup had been in talks with OpenAI for a $3 billion acquisition deal, CNBC reported in April. OpenAI and Windsurf had entered into exclusivity in order to finalize an acquisition, but the period has expired, an OpenAI spokesperson said.

The move ratchets up the talent war in AI, particularly among prominent companies. Meta has made lucrative job offers to several employees at OpenAI in recent weeks. Most notably, the Facebook parent added Scale AI founder Alexandr Wang to lead its AI strategy as part of a $14.3 billion investment in his startup.

Douglas Chen, another Windsurf co-founder, will be among those joining Google in the deal, Jeff Wang, the startup's new interim CEO and its head of business for the past two years, wrote in a post on X.

"Most of Windsurf's world-class team will continue to build the Windsurf product with the goal of maximizing its impact in the enterprise," Wang wrote.

Windsurf has become more popular this year as an option for so-called vibe coding, which is the process of using new age AI tools to write code. Developers and non-developers have embraced the concept, leading to more revenue for Windsurf and competitors, such as Cursor, which OpenAI also looked at buying. All the interest has led investors to assign higher valuations to the startups.

This isn't the first time Google has hired select people from a startup. It did the same with Character.AI last summer. Amazon and Microsoft have also absorbed AI talent in this fashion, with the Adept and Inflection deals, respectively.

Microsoft is pushing an agent mode in its Visual Studio Code editor for vibe coding. In April, Microsoft CEO Satya Nadella said AI is composing as much as 30% of his company's code.

The Verge reported the Google-Windsurf deal earlier on Friday.

WATCH: Google pushes "AI Mode" on homepage

---

### Google
 hires Windsurf CEO Varun Mohan, others in $2.4 billion AI talent deal (3 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cnbc.com%2F2025%2F07%2F11%2Fgoogle-windsurf-ceo-varun-mohan-latest-ai-talent-deal-.html%3Futm_source=tldrnewsletter/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/e7OE18VbvzW2NrPJrEl6DcCO7GayS13e9xG6Y5uVY90=413
**TLDR Summary:** Google has signed an agreement to hire Varum Mohan, the co-founder of AI coding startup, Windsurf, and other senior Windsurf research and development employees. Google will not invest in Windsurf, but it will receive a nonexclusive license to certain Windsurf technology. Windsurf is free to license its technology to others. Google is paying $2.4 billion for the deal. Windsurf had been in talks with OpenAI for a $3 billion acquisition deal, but the finalization period expired.
**Full Article Content:**
Google on Friday made the latest splash in the AI talent wars, announcing an agreement to bring in Varun Mohan, co-founder and CEO of artificial intelligence coding startup Windsurf.

As part of the deal, Google will also hire other senior Windsurf research and development employees. Google is not investing in Windsurf, but the search giant will take a nonexclusive license to certain Windsurf technology, according to a person familiar with the matter. Windsurf remains free to license its technology to others.

The person familiar with the deal says Google is paying $2.4 billion in licensing fees and for compensation.

"We're excited to welcome some top AI coding talent from Windsurf's team to Google DeepMind to advance our work in agentic coding," a Google spokesperson wrote in an email. "We're excited to continue bringing the benefits of Gemini to software developers everywhere."

The deal between Google and Windsurf comes after the AI coding startup had been in talks with OpenAI for a $3 billion acquisition deal, CNBC reported in April. OpenAI and Windsurf had entered into exclusivity in order to finalize an acquisition, but the period has expired, an OpenAI spokesperson said.

The move ratchets up the talent war in AI, particularly among prominent companies. Meta has made lucrative job offers to several employees at OpenAI in recent weeks. Most notably, the Facebook parent added Scale AI founder Alexandr Wang to lead its AI strategy as part of a $14.3 billion investment in his startup.

Douglas Chen, another Windsurf co-founder, will be among those joining Google in the deal, Jeff Wang, the startup's new interim CEO and its head of business for the past two years, wrote in a post on X.

"Most of Windsurf's world-class team will continue to build the Windsurf product with the goal of maximizing its impact in the enterprise," Wang wrote.

Windsurf has become more popular this year as an option for so-called vibe coding, which is the process of using new age AI tools to write code. Developers and non-developers have embraced the concept, leading to more revenue for Windsurf and competitors, such as Cursor, which OpenAI also looked at buying. All the interest has led investors to assign higher valuations to the startups.

This isn't the first time Google has hired select people from a startup. It did the same with Character.AI last summer. Amazon and Microsoft have also absorbed AI talent in this fashion, with the Adept and Inflection deals, respectively.

Microsoft is pushing an agent mode in its Visual Studio Code editor for vibe coding. In April, Microsoft CEO Satya Nadella said AI is composing as much as 30% of his company's code.

The Verge reported the Google-Windsurf deal earlier on Friday.

WATCH: Google pushes "AI Mode" on homepage

---

### SpaceX
 to Invest $2 Billion Into Elon Musk's xAI (3 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FsLLx6U/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/PV_Kj4fW1d0McdtTxbz8UilnS3VFaDIF_QH-A4sxHfo=413
**TLDR Summary:** SpaceX agreed to invest $2 billion into xAI as part of xAI's $5 billion equity fundraise announced by Morgan Stanley last month. This is not the first time Elon Musk has mobilized his business empire to boost the AI startup. More business partnerships between SpaceX and xAI are likely in the future. SpaceX recently had more than $3 billion in cash on hand - the company rarely makes investments in outside ventures.
**Full Article Content:**
[Scraping failed for this URL. Error: Article `download()` failed with 401 Client Error: HTTP Forbidden for url: https://www.wsj.com/tech/spacex-to-invest-2-billion-into-elon-musks-xai-413934de?st=RGF5bY&reflink=desktopwebshare_permalink&utm_source=tldrnewsletter on URL https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FsLLx6U/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/PV_Kj4fW1d0McdtTxbz8UilnS3VFaDIF_QH-A4sxHfo=413]

---

## Science & Futuristic Technology

### Lucid
 absolutely smashes Guinness World Record for the longest EV drive on a single charge (4 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnewatlas.com%2Fautomotive%2Flucid-air-electric-vehicle-distance-single-charge-record%2F%3Futm_source=tldrnewsletter/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/M-eqLtYkqwK_PiDSU09zYh7-zd6fc0QG-dDWwet_7Y8=413
**TLDR Summary:** Lucid Motors just set a Guinness World Record for the longest journey by an electric car on a single charge using its Air Grand Touring car, covering a distance of around 768 miles. The previous record of 649 miles was achieved by the Mercedes-Benz ETS450+ in June 2025. Lucid Motors' EV traveled between St. Moritz, Switzerland, and Munich, Germany through highways, secondary roads, and alpine roads. The Air Grand Touring, starting at $112,650, has two all-wheel drive electric motors with a combined system output of 611 kW (819 horsepower) and 1,200 Nm (885 lb-ft) of torque.
**Full Article Content:**
Let me throw two terms at you: inadequate public charging infrastructure and range anxiety. Any guesses what I'm talking about? Yup, the Achilles heel of electric vehicles. Solve these, and you're probably looking at widespread EV adoption.

Lucid Motors might have put both problems in the rear-view mirror by setting a Guinness World Record for the longest journey by an electric car on a single charge.

The milestone was achieved using its Air Grand Touring car, where the crew from Lucid covered a distance of 1,205 km (~749 miles). In doing so, Lucid broke the 1,045-km (649-mile) record previously achieved by the Mercedes-Benz EQS450+ in June 2025 by the Japanese car website webCG.

The Lucid Air Grand Touring conducted a record-breaking journey of 1,205 kilometers (~749 miles) between St. Moritz, Switzerland and Munich, Germany Lucid Motors

The electric vehicle (EV) covered this journey between St. Moritz, Switzerland, and Munich, Germany, traveling through highways, secondary roads, and alpine roads – all without a single halt for charging. Given that the vehicle has a 960-km (596-mile) WLTP range, my guess is that the test team must have made good use of favorable road and weather conditions to make the feat possible.

With a net elevation decrease of just over 1,310 m (about 4,300 ft) throughout the drive, the EV most certainly benefited from regenerative braking, a rather useful feature that turns downhill momentum back into battery power. Lucid has yet to release official data like average speed or total drive time, but what is apparent is that this was not a high-speed dash but rather a well-planned route to achieve one impressive result.

Umit Sabanci, a London-based entrepreneur with the odd hobby of shattering world records, was behind the wheel of the Lucid. He last partnered with the company in June 2024 when he broke the record driving to the most countries without recharging, traveling 912 km (567 miles) south through the Netherlands, Belgium, Luxembourg, Germany, France, Switzerland, Austria, Liechtenstein, and Italy.

“When I completed the nine-country journey in 2024, it was just the beginning,” said Sabanci. “This new achievement takes that journey even further. I’m proud to be part of a movement that proves electric mobility isn’t just the future; it’s already redefining what’s possible today.”

The Air Grand Touring has two all-wheel drive electric motors with a combined system output of 819 horsepower (611 kW) and 885 lb.ft (1,200 Nm) of torque Lucid Motors

The Air Grand Touring has two all-wheel drive electric motors with a combined system output of 611 kW (819 horsepower) and 1,200 Nm (885 lb.ft) of torque. Power is provided by an NMC battery, which has a gross energy capacity of 117 kWh (112 kWh usable). Best of all, it can go from 0-60 mph in just three seconds flat.

The EV's 900-volt electrical architecture allows for up to 300 kW of DC rapid charging and 19.2 kW of slower AC charging. According to Lucid, if the car is linked to a sufficiently strong DC fast-charger, its battery can be topped up with 350 km (217 miles) of range in roughly 15 minutes. In real-world usage, it would likely take more than an hour to fully charge an Air GT via a Level 3 charger.

For reference, the almost half-priced BMW i4 and jazzy Porsche Taycan offer less than half the WLTP range of the Lucid Air GT. So, it’s not like there’s a head-to-head competition out there. Lucid is miles ahead in its class (pun intended!)

Starting at $112,650, the Air Grand Touring is among the most luxurious sedans on the market right now Lucid Motors

Starting at US$112,650, the Air Grand Touring is among the most luxurious sedans on the market right now. But as you can see, it comes at a price. Still, knowing that there is technology to conquer range anxiety is comforting.

It might take a while, but there's no reason why we can't expect such range figures from reasonably priced EVs in the near future. If anything, it's a signal of the things to come in the EV space.

Source: Lucid

---

### Lucid
 absolutely smashes Guinness World Record for the longest EV drive on a single charge (4 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fnewatlas.com%2Fautomotive%2Flucid-air-electric-vehicle-distance-single-charge-record%2F%3Futm_source=tldrnewsletter/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/M-eqLtYkqwK_PiDSU09zYh7-zd6fc0QG-dDWwet_7Y8=413
**TLDR Summary:** Lucid Motors just set a Guinness World Record for the longest journey by an electric car on a single charge using its Air Grand Touring car, covering a distance of around 768 miles. The previous record of 649 miles was achieved by the Mercedes-Benz ETS450+ in June 2025. Lucid Motors' EV traveled between St. Moritz, Switzerland, and Munich, Germany through highways, secondary roads, and alpine roads. The Air Grand Touring, starting at $112,650, has two all-wheel drive electric motors with a combined system output of 611 kW (819 horsepower) and 1,200 Nm (885 lb-ft) of torque.
**Full Article Content:**
Let me throw two terms at you: inadequate public charging infrastructure and range anxiety. Any guesses what I'm talking about? Yup, the Achilles heel of electric vehicles. Solve these, and you're probably looking at widespread EV adoption.

Lucid Motors might have put both problems in the rear-view mirror by setting a Guinness World Record for the longest journey by an electric car on a single charge.

The milestone was achieved using its Air Grand Touring car, where the crew from Lucid covered a distance of 1,205 km (~749 miles). In doing so, Lucid broke the 1,045-km (649-mile) record previously achieved by the Mercedes-Benz EQS450+ in June 2025 by the Japanese car website webCG.

The Lucid Air Grand Touring conducted a record-breaking journey of 1,205 kilometers (~749 miles) between St. Moritz, Switzerland and Munich, Germany Lucid Motors

The electric vehicle (EV) covered this journey between St. Moritz, Switzerland, and Munich, Germany, traveling through highways, secondary roads, and alpine roads – all without a single halt for charging. Given that the vehicle has a 960-km (596-mile) WLTP range, my guess is that the test team must have made good use of favorable road and weather conditions to make the feat possible.

With a net elevation decrease of just over 1,310 m (about 4,300 ft) throughout the drive, the EV most certainly benefited from regenerative braking, a rather useful feature that turns downhill momentum back into battery power. Lucid has yet to release official data like average speed or total drive time, but what is apparent is that this was not a high-speed dash but rather a well-planned route to achieve one impressive result.

Umit Sabanci, a London-based entrepreneur with the odd hobby of shattering world records, was behind the wheel of the Lucid. He last partnered with the company in June 2024 when he broke the record driving to the most countries without recharging, traveling 912 km (567 miles) south through the Netherlands, Belgium, Luxembourg, Germany, France, Switzerland, Austria, Liechtenstein, and Italy.

“When I completed the nine-country journey in 2024, it was just the beginning,” said Sabanci. “This new achievement takes that journey even further. I’m proud to be part of a movement that proves electric mobility isn’t just the future; it’s already redefining what’s possible today.”

The Air Grand Touring has two all-wheel drive electric motors with a combined system output of 819 horsepower (611 kW) and 885 lb.ft (1,200 Nm) of torque Lucid Motors

The Air Grand Touring has two all-wheel drive electric motors with a combined system output of 611 kW (819 horsepower) and 1,200 Nm (885 lb.ft) of torque. Power is provided by an NMC battery, which has a gross energy capacity of 117 kWh (112 kWh usable). Best of all, it can go from 0-60 mph in just three seconds flat.

The EV's 900-volt electrical architecture allows for up to 300 kW of DC rapid charging and 19.2 kW of slower AC charging. According to Lucid, if the car is linked to a sufficiently strong DC fast-charger, its battery can be topped up with 350 km (217 miles) of range in roughly 15 minutes. In real-world usage, it would likely take more than an hour to fully charge an Air GT via a Level 3 charger.

For reference, the almost half-priced BMW i4 and jazzy Porsche Taycan offer less than half the WLTP range of the Lucid Air GT. So, it’s not like there’s a head-to-head competition out there. Lucid is miles ahead in its class (pun intended!)

Starting at $112,650, the Air Grand Touring is among the most luxurious sedans on the market right now Lucid Motors

Starting at US$112,650, the Air Grand Touring is among the most luxurious sedans on the market right now. But as you can see, it comes at a price. Still, knowing that there is technology to conquer range anxiety is comforting.

It might take a while, but there's no reason why we can't expect such range figures from reasonably priced EVs in the near future. If anything, it's a signal of the things to come in the EV space.

Source: Lucid

---

### A Never-Ending
 Supply of Drones Has Frozen the Front Lines in Ukraine (7 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FRE9OcO/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/rZjnSAMkpKx953981gR_mD-TAKzy7ceAoUyoUYVMvFM=413
**TLDR Summary:** Drones are dominating the battlefield in the fight for Ukraine. Both sides have hundreds of them constantly in the air across the 750-mile front line. Drones can lay mines, deliver ammunition and medication, and even evacuate wounded or dead soldiers. They are being used to spot movement along the front line and dispatched to strike enemy troops and vehicles. Drones are so cheap to make that both sides can expend them on any target, including individual soldiers. Due to their size and speed, they are difficult to shoot down, so the main defense against them has been electronic jamming systems - but these can be overcome with fiber-optic drones.
**Full Article Content:**
[Scraping failed for this URL. Error: Article `download()` failed with 401 Client Error: HTTP Forbidden for url: https://www.wsj.com/world/europe/a-never-ending-supply-of-drones-has-frozen-the-front-lines-in-ukraine-ae29c581?mod=tech_lead_story&utm_source=tldrnewsletter on URL https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FRE9OcO/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/rZjnSAMkpKx953981gR_mD-TAKzy7ceAoUyoUYVMvFM=413]

---

## Programming, Design & Data Science

### Announcing
 GenAI Processors: Build powerful and flexible Gemini applications (6 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevelopers.googleblog.com%2Fen%2Fgenai-processors%2F%3Futm_source=tldrnewsletter/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/ZY2HwecRtyVYSh1vrIhMAHv2gs1jGp86opwtwUghjSo=413
**TLDR Summary:** Google DeepMind has released an open-source Python library designed to bring structure and simplicity to the changes of building AI applications with large language models. GenAI Processors provides an abstraction layer that defines a consistent interface for everything from input handling and pre-processing to model calls and output processing. The stream-based API allows for seamless chaining and composition of different operations. GenAI Processors helps developers write responsive applications without making code more complex.
**Full Article Content:**
Building sophisticated AI applications with Large Language Models (LLMs), especially those handling multimodal input and requiring real-time responsiveness, often feels like assembling a complex puzzle: you're stitching together diverse data processing steps, asynchronous API calls, and custom logic. As complexity grows, this can lead to brittle, hard-to-maintain code. Today, we're introducing GenAI Processors, a new open-source Python library from Google DeepMind designed to bring structure and simplicity to these challenges. GenAI Processors provides an abstraction layer, defining a consistent Processor interface for everything from input handling and pre-processing to model calls and output processing. At its core, GenAI Processors treat all input and output as asynchronous streams of ProcessorParts (i.e. two-way aka bidirectional streaming). Think of it as standardized data parts (e.g., a chunk of audio, a text transcription, an image frame) flowing through your pipeline along with associated metadata. This stream-based API allows for seamless chaining and composition of different operations, from low-level data manipulation to high-level model calls.

The GenAI Processors library is designed to optimize the concurrent execution of a Processor. Any part in this example of execution flow can be generated concurrently when all its ancestors in the graph are computed, e.g. `c'12` can be generated concurrently to `a’1`. The flow maintains the ordering of the output stream with respect to the input stream and will be executed to minimize Time To First Token (prefer `a12` to `d12` whenever possible). This concurrency optimization is done under the hood: applying a Processor to a stream of input will automatically trigger this concurrent execution whenever possible.

For example, you can easily build a "Live Agent" capable of processing audio and video streams in real-time using the Gemini Live API with just a few lines of code. In the following example, notice how input sources and processing steps are combined using the + operator, creating a clear data flow (full code on GitHub):

from genai_processors.core import audio_io, live_model, video # Input processor: combines camera streams and audio streams input_processor = video.VideoIn() + audio_io.PyAudioIn(...) # Output processor: plays the audio parts. Handles interruptions and pauses # audio output when the user is speaking. play_output = audio_io.PyAudioOut(...) # Gemini Live API processor live_processor = live_model.LiveProcessor(...) # Compose the agent: mic+camera -> Gemini Live API -> play audio live_processor = live_model.LiveProcessor(...) live_agent = input_processor + live_processor + play_output async for part in live_agent(streams.endless_stream()): # Process the output parts (e.g., print transcription, model output, metadata) print(part) Python Copied

You can also build your own Live agent, leveraging a standard text-based LLM, using the bidirectional streaming capability of the GenAI Processor library and the Google Speech API (full code on GitHub):

from genai_processors.core import genai_model, realtime, speech_to_text, text_to_speech # Input processor: gets input from audio in (mic) and transcribes into text input_processor = audio_io.PyAudioIn(...) + speech_to_text.SpeechToText(... ) play_output = audio_io.PyAudioOut(...) # Main model that will be used to generate the response. genai_processor = genai_model.GenaiModel(...), # TTS processor that will be used to convert the text response to audio. Note # the rate limit audio processor that will be used to stream back small audio # chunks to the client at the same rate as how they are played back. tts = text_to_speech.TextToSpeech(...) + rate_limit_audio.RateLimitAudio(...) # Creates an agent as: # mic -> speech to text -> text conversation -> text to speech -> play audio live_agent = ( input_processor + realtime.LiveModelProcessor(turn_processor=genai_processor + tts) + play_output ) async for part in live_agent(streams.endless_stream()): … Python Copied

We anticipate a growing need for proactive LLM applications where responsiveness is critical. Even for non-streaming use cases, processing data as soon as it is available can significantly reduce latency and time to first token (TTFT), which is essential for building a good user experience. While many LLM APIs prioritize synchronous, simplified interfaces, GenAI Processors – by leveraging native Python features – offer a way for writing responsive applications without making code more complex. Trip planner and Research Agent examples demonstrate how turn-based agents can use the concurrency feature of GenAI Processors to increase responsiveness.

Core design principles At the heart of GenAI Processors is the concept of a Processor : a fundamental building block that encapsulates a specific unit of work. It takes a stream of inputs, performs an operation, and outputs a stream of results. This simple, consistent API is a cornerstone of the library's power and flexibility. Here's a look at the core design decisions and their benefits for developers: Modular design: Break down complex workflows into self-contained Processor units. This ensures code reusability, testability, and significantly simplifies maintaining intricate pipelines. Asynchronous & concurrent: Fully leverages Python's asyncio for efficient handling of I/O-bound and compute-bound tasks. This enables responsive applications without manual threading or complex concurrency management. Integrated with Gemini API: Dedicated processors like GenaiModel (for turn-based interaction) and LiveProcessor (for real-time streaming) simplify interaction with the Gemini API, including the complexities of the Live API. This reduces boilerplate and accelerates integration. Extensible: Easily create custom processors by inheriting from base classes or using decorators. Integrate your own data processing logic, external APIs, or specialized operations seamlessly into your pipelines. Unified multimodal handling: The ProcessorPart wrapper provides a consistent interface for handling diverse data types (text, images, audio, JSON, etc.) within the pipeline. Stream manipulation utilities: Built-in utilities for splitting, concatenating, and merging asynchronous streams. This provides fine-grained control over data flow within complex pipelines.

Getting started Getting started with GenAI Processors is straightforward. You can install it with pip:

pip install genai-processors Python Copied

---

### Announcing
 GenAI Processors: Build powerful and flexible Gemini applications (6 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdevelopers.googleblog.com%2Fen%2Fgenai-processors%2F%3Futm_source=tldrnewsletter/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/ZY2HwecRtyVYSh1vrIhMAHv2gs1jGp86opwtwUghjSo=413
**TLDR Summary:** Google DeepMind has released an open-source Python library designed to bring structure and simplicity to the changes of building AI applications with large language models. GenAI Processors provides an abstraction layer that defines a consistent interface for everything from input handling and pre-processing to model calls and output processing. The stream-based API allows for seamless chaining and composition of different operations. GenAI Processors helps developers write responsive applications without making code more complex.
**Full Article Content:**
Building sophisticated AI applications with Large Language Models (LLMs), especially those handling multimodal input and requiring real-time responsiveness, often feels like assembling a complex puzzle: you're stitching together diverse data processing steps, asynchronous API calls, and custom logic. As complexity grows, this can lead to brittle, hard-to-maintain code. Today, we're introducing GenAI Processors, a new open-source Python library from Google DeepMind designed to bring structure and simplicity to these challenges. GenAI Processors provides an abstraction layer, defining a consistent Processor interface for everything from input handling and pre-processing to model calls and output processing. At its core, GenAI Processors treat all input and output as asynchronous streams of ProcessorParts (i.e. two-way aka bidirectional streaming). Think of it as standardized data parts (e.g., a chunk of audio, a text transcription, an image frame) flowing through your pipeline along with associated metadata. This stream-based API allows for seamless chaining and composition of different operations, from low-level data manipulation to high-level model calls.

The GenAI Processors library is designed to optimize the concurrent execution of a Processor. Any part in this example of execution flow can be generated concurrently when all its ancestors in the graph are computed, e.g. `c'12` can be generated concurrently to `a’1`. The flow maintains the ordering of the output stream with respect to the input stream and will be executed to minimize Time To First Token (prefer `a12` to `d12` whenever possible). This concurrency optimization is done under the hood: applying a Processor to a stream of input will automatically trigger this concurrent execution whenever possible.

For example, you can easily build a "Live Agent" capable of processing audio and video streams in real-time using the Gemini Live API with just a few lines of code. In the following example, notice how input sources and processing steps are combined using the + operator, creating a clear data flow (full code on GitHub):

from genai_processors.core import audio_io, live_model, video # Input processor: combines camera streams and audio streams input_processor = video.VideoIn() + audio_io.PyAudioIn(...) # Output processor: plays the audio parts. Handles interruptions and pauses # audio output when the user is speaking. play_output = audio_io.PyAudioOut(...) # Gemini Live API processor live_processor = live_model.LiveProcessor(...) # Compose the agent: mic+camera -> Gemini Live API -> play audio live_processor = live_model.LiveProcessor(...) live_agent = input_processor + live_processor + play_output async for part in live_agent(streams.endless_stream()): # Process the output parts (e.g., print transcription, model output, metadata) print(part) Python Copied

You can also build your own Live agent, leveraging a standard text-based LLM, using the bidirectional streaming capability of the GenAI Processor library and the Google Speech API (full code on GitHub):

from genai_processors.core import genai_model, realtime, speech_to_text, text_to_speech # Input processor: gets input from audio in (mic) and transcribes into text input_processor = audio_io.PyAudioIn(...) + speech_to_text.SpeechToText(... ) play_output = audio_io.PyAudioOut(...) # Main model that will be used to generate the response. genai_processor = genai_model.GenaiModel(...), # TTS processor that will be used to convert the text response to audio. Note # the rate limit audio processor that will be used to stream back small audio # chunks to the client at the same rate as how they are played back. tts = text_to_speech.TextToSpeech(...) + rate_limit_audio.RateLimitAudio(...) # Creates an agent as: # mic -> speech to text -> text conversation -> text to speech -> play audio live_agent = ( input_processor + realtime.LiveModelProcessor(turn_processor=genai_processor + tts) + play_output ) async for part in live_agent(streams.endless_stream()): … Python Copied

We anticipate a growing need for proactive LLM applications where responsiveness is critical. Even for non-streaming use cases, processing data as soon as it is available can significantly reduce latency and time to first token (TTFT), which is essential for building a good user experience. While many LLM APIs prioritize synchronous, simplified interfaces, GenAI Processors – by leveraging native Python features – offer a way for writing responsive applications without making code more complex. Trip planner and Research Agent examples demonstrate how turn-based agents can use the concurrency feature of GenAI Processors to increase responsiveness.

Core design principles At the heart of GenAI Processors is the concept of a Processor : a fundamental building block that encapsulates a specific unit of work. It takes a stream of inputs, performs an operation, and outputs a stream of results. This simple, consistent API is a cornerstone of the library's power and flexibility. Here's a look at the core design decisions and their benefits for developers: Modular design: Break down complex workflows into self-contained Processor units. This ensures code reusability, testability, and significantly simplifies maintaining intricate pipelines. Asynchronous & concurrent: Fully leverages Python's asyncio for efficient handling of I/O-bound and compute-bound tasks. This enables responsive applications without manual threading or complex concurrency management. Integrated with Gemini API: Dedicated processors like GenaiModel (for turn-based interaction) and LiveProcessor (for real-time streaming) simplify interaction with the Gemini API, including the complexities of the Live API. This reduces boilerplate and accelerates integration. Extensible: Easily create custom processors by inheriting from base classes or using decorators. Integrate your own data processing logic, external APIs, or specialized operations seamlessly into your pipelines. Unified multimodal handling: The ProcessorPart wrapper provides a consistent interface for handling diverse data types (text, images, audio, JSON, etc.) within the pipeline. Stream manipulation utilities: Built-in utilities for splitting, concatenating, and merging asynchronous streams. This provides fine-grained control over data flow within complex pipelines.

Getting started Getting started with GenAI Processors is straightforward. You can install it with pip:

pip install genai-processors Python Copied

---

### I messed
 up my Google PM Vibe Coding Interview (2 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FjGtQYy/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/R7qWm2S-xpadmrc1coTD9LY2ZWtPkiB-0_0Cbt-x2EM=413
**TLDR Summary:** A Reddit user attended the second round of their Google Product Management interview and was surprised when it turned out to be a vibe coding interview. This caused them to become flustered and underperform - they had the knowledge, but the stress caused them to blank on the exact implementation. Those interviewing for similar roles should take this post as a warning - be prepared for the possibility of a vibe coding interview format. The Reddit user was interviewing for an L5 PM role that had a strong focus on AI/ML.
**Full Article Content:**
[Scraping failed for this URL. Error: Article `download()` failed with 403 Client Error: Blocked for url: https://old.reddit.com/r/ProductManagement/comments/1lw9r9h/i_messed_up_my_google_pm_vibe_coding_interview/?utm_source=tldrnewsletter on URL https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FjGtQYy/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/R7qWm2S-xpadmrc1coTD9LY2ZWtPkiB-0_0Cbt-x2EM=413]

---

### Tim Cook
 Isn't Going Anywhere Soon, But an Apple Shake-Up Looms (16 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FI3eO0F/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/0SGMF7fuyIxZZOuMlk-_M1q8AMjS3VOmf7q9uLFqc58=413
**TLDR Summary:** Apple's Chief Executive Officer Tim Cook is probably not going to leave anytime soon despite being mired in Apple's biggest crisis in years. Cook's longtime No. 2, Jeff Williams, just announced plans to retire, so there's no immediate successor ready to take the helm, and there haven't been signs that Cook has begun the process of grooming a replacement. The board also doesn't feel the need to make a change - while shares are down 16% this year, they've surged about 1,500% since he became CEO in 2011. While Cook is staying, the company has started a broad management shake-up - roughly half of Cook's direct reports could depart within the next few years.
**Full Article Content:**
The departure of Apple’s chief operating officer is just the beginning of a broader management overhaul — though Tim Cook is staying put for the foreseeable future. Also: The company is scaling back the iOS 26 Liquid Glass revamp; a look at upcoming devices; and Meta poaches Apple’s top AI models executive.

Last time in Power On: Apple should sell a smart ring to expand its fitness wearables lineup.

---

### Tim Cook
 Isn't Going Anywhere Soon, But an Apple Shake-Up Looms (16 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FI3eO0F/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/0SGMF7fuyIxZZOuMlk-_M1q8AMjS3VOmf7q9uLFqc58=413
**TLDR Summary:** Apple's Chief Executive Officer Tim Cook is probably not going to leave anytime soon despite being mired in Apple's biggest crisis in years. Cook's longtime No. 2, Jeff Williams, just announced plans to retire, so there's no immediate successor ready to take the helm, and there haven't been signs that Cook has begun the process of grooming a replacement. The board also doesn't feel the need to make a change - while shares are down 16% this year, they've surged about 1,500% since he became CEO in 2011. While Cook is staying, the company has started a broad management shake-up - roughly half of Cook's direct reports could depart within the next few years.
**Full Article Content:**
The departure of Apple’s chief operating officer is just the beginning of a broader management overhaul — though Tim Cook is staying put for the foreseeable future. Also: The company is scaling back the iOS 26 Liquid Glass revamp; a look at upcoming devices; and Meta poaches Apple’s top AI models executive.

Last time in Power On: Apple should sell a smart ring to expand its fitness wearables lineup.

---

### Hypercapitalism
 and the AI talent wars (18 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FdwXeL8/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/70aPxoGpF9F9BrRsaVo0kmL6GavQWUqIIPD-A04zAIs=413
**TLDR Summary:** The hypercapitalist AI talent wars will rewrite employment contracts and investment norms, concentrate returns, and raise the bar for mission and capital required to create great new companies. The social contracts between employees, startups, and investors must be rewritten. Founders must prepare themselves for the step-function increase in mercenary firepower. The M&A talent war is just beginning - as the numbers get bigger for talent and companies, all sides need to reimagine the social contract.
**Full Article Content:**
JavaScript is not available.

We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.

Help Center

---

## Quick Links

### Craving
 more AI in your inbox? (Sponsor)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftldr.tech%2Fai%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=quicklinks07142025/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/D6fo5U1wN7dWZG1vo0SPWwcnwRm5etMkPSKBonqxpMI=413
**TLDR Summary:** TLDR AI is your daily fix of LLMs, GenAI, and deep learning goodness. Same TLDR format. Still free. Subscribe now.
**Full Article Content:**
🧠

TLDR AI

Get smarter about AI in 5 minutes

The most important AI, ML, and data science news in a free daily email.

Sign Up

No spam. Unsubscribe at any time in one click.

---

### Craving
 more AI in your inbox? (Sponsor)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftldr.tech%2Fai%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=quicklinks07142025/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/D6fo5U1wN7dWZG1vo0SPWwcnwRm5etMkPSKBonqxpMI=413
**TLDR Summary:** TLDR AI is your daily fix of LLMs, GenAI, and deep learning goodness. Same TLDR format. Still free. Subscribe now.
**Full Article Content:**
🧠

TLDR AI

Get smarter about AI in 5 minutes

The most important AI, ML, and data science news in a free daily email.

Sign Up

No spam. Unsubscribe at any time in one click.

---

### Intel
 CEO says it's "too late" for them to catch up with AI competition (5 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.tomshardware.com%2Ftech-industry%2Fintel-ceo-says-its-too-late-for-them-to-catch-up-with-ai-competition-claims-intel-has-fallen-out-of-the-top-10-semiconductor-companies-as-the-firm-lays-off-thousands-across-the-world%3Futm_source=tldrnewsletter/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/DZdD2v_VkDPu72hYHxwcKR41FdIhY7frWyj8NzrcviQ=413
**TLDR Summary:** Intel has been laying off thousands of workers around the world as it faces giant losses and skyrocketing costs.
**Full Article Content:**
Intel has been in a dire state these past few years, with seemingly nothing going right. Its attempt to modernize x86 with a hybrid big.LITTLE architecture, à la ARM, failed to make a meaningful impact in terms of market share gains, only made worse by last-gen's Arrow Lake chips barely registering a response against AMD’s lineup. On the GPU front, the Blue Team served an undercooked product far too late that, while not entirely hopeless, was nowhere near enough to challenge the industry’s dominant players. All of this compounds into a grim reality, seemingly confirmed by new CEO Lip-Bu Tan in a leaked internal conversation today.

According to OregonTech, it's borderline a fight for survival for the once-great American innovation powerhouse as it struggles to even acknowledge being among the top contenders anymore. Despite Tan's insistence, Intel would still rank fairly well given its extensive legacy. While companies like AMD, Nvidia, Apple, TSMC, and even Samsung might be more successful today, smaller chipmakers like Broadcom, MediaTek, Micron, and SK Hynix are not above the Blue Team in terms of sheer impact. Regardless, talking to employees around the world in a Q&A session, Intel's CEO allegedly shared these bleak words: "Twenty, 30 years ago, we were really the leader. Now I think the world has changed. We are not in the top 10 semiconductor companies."

As evident from the quote, this is a far cry from a few decades ago when Intel essentially held a monopoly over the CPU market, making barely perceptible upgrades each generation in order to sustain its dominance. At one time, Intel was so powerful that it considered acquiring Nvidia for $20 billion. The GPU maker is now worth $4 trillion.

It never saw AMD as an honorable competitor until it was too late, and Ryzen pulled the carpet from underneath the Blue Team's feet. Now, more people choose to build an AMD system than ever before. Not only that, but AMD also powers your favorite handhelds like the Steam Deck and Rog Ally X, alongside the biggest consoles: Xbox Series and PlayStation 5. AMD works closely with TSMC, another one of Intel's competitors, as the company makes its own chips in-house.

This vertical alignment was once a core strength for the firm, but it has turned into more of a liability these days. Faltering nodes that can't quite match the prowess of Taiwan have arguably held back Intel's processors from reaching their full potential. In fact, starting in 2023, the company tasked TSMC with manufacturing the GPU tile on its Meteor Lake chips. This partnership extended to TSMC, essentially making the entire compute tile for Lunar Lake—and now, in 2025, roughly 30% of fabrication has been outsourced to TSMC. A long-overdue admission of failure that could've been prevented had Intel been allowed to make its leading-edge CPUs with external manufacturing in mind from the start. Ultimately its own foundry was the limiting factor.

As such, Intel has been laying off thousands across the world in a bid to cut costs. Costs have skyrocketed due to high R&D spending for future nodes, and the company faces a $16 billion loss in Q3 last year. Intel's resurrection has to be a "marathon," said Tan, as he hopes to turn around the company culture and "be humble" in listening to shifting demands of the industry. Intel wants to be more like AMD and NVIDIA, who are faster, meaner, and more ruthless competitors these days, especially with the advent of AI. Of course, artificial intelligence has been around for a while, but it wasn't until OpenAI's ChatGPT that a second big bang occurred, ushering in a new era of machine learning. An era almost entirely powered by Nvidia's data center GPUs, highlighting another sector where Intel failed to capitalize on its position.

"On training, I think it is too late for us," Lip-Bu Tan remarked. Intel instead plans to shift its focus toward edge AI, aiming to bring AI processing directly to devices like PCs rather than relying on cloud-based compute. Tan also highlighted agentic AI—an emerging field where AI systems can act autonomously without constant human input—as a key growth area. He expressed optimism that recent high-level hires could help steer Intel back into relevance in AI, hinting that more talent acquisitions are on the way. “Stay tuned. A few more people are coming on board,” said Tan. At this point, Nvidia is simply too far ahead to catch up to, so it's almost exciting to see Intel change gears and look to close the gap in a different way.

Stay On the Cutting Edge: Get the Tom's Hardware Newsletter Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors

That being said, Intel now lags behind in data center CPUs, too, where AMD's EPYC lineup has overtaken them in the past year, further dwindling the company's confidence. Additionally, last year, Intel's board forced former CEO Pat Gelsinger out of the company and replaced him with Lip-Bu Tan, who appears to have a distinctly different, more streamlined vision for the company. Instead of focusing on several different facets, such as CPU, GPU, foundry, and more, at once, Lip wants to hone in on what the company can do well at one time.

This development follows long-standing rumors of Intel splitting in two and forming a new foundry division that would act as an independent subsidiary, turning the main Intel into a fabless chipmaker. Both AMD and Apple, Intel's rivals in the CPU market, operate like this, and Nvidia has also always used TSMC or Samsung to build its graphics cards. It would be interesting to see the Blue Team shed off weight and move like a free animal in the biome. However, it's too early to speculate given that 18A, Intel's proposed savior, is still a year away.

---

### Musk's
 xAI seeks up to $200 billion valuation in next funding round (2 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FEYfT6e/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/TbfLGHXDx9E0cGGHLO4i0LW_3ev0YVI8amJWZN5qRoM=413
**TLDR Summary:** Saudi Arabia's PIF sovereign wealth fund is expected to play a large role in the deal.
**Full Article Content:**
[Scraping failed for this URL. Error: Article `download()` failed with 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/business/musks-xai-seeks-up-200-billion-valuation-next-fundraising-ft-reports-2025-07-11/?utm_source=tldrnewsletter on URL https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FEYfT6e/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/TbfLGHXDx9E0cGGHLO4i0LW_3ev0YVI8amJWZN5qRoM=413]

---

### watchfiles
 (GitHub Repo)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fgithub.com%2Fsamuelcolvin%2Fwatchfiles%3Futm_source=tldrnewsletter/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/OxwEbri8GgMzQ6uJtKr7chRN_J2vASCizhQo5ahFzZg=413
**TLDR Summary:** watchfiles is a simple, modern, and high-performance file watching tool for Python.
**Full Article Content:**
watchfiles

Simple, modern and high performance file watching and code reload in python.

Documentation: watchfiles.helpmanual.io

Source Code: github.com/samuelcolvin/watchfiles

Underlying file system notifications are handled by the Notify rust library.

This package was previously named "watchgod", see the migration guide for more information.

Installation

watchfiles requires Python 3.9 - 3.14.

pip install watchfiles

Binaries are available for most architectures on Linux, MacOS and Windows (learn more).

Otherwise, you can install from source which requires Rust stable to be installed.

Usage

Here are some examples of what watchfiles can do:

watch Usage

from watchfiles import watch for changes in watch ( './path/to/dir' ): print ( changes )

See watch docs for more details.

awatch Usage

import asyncio from watchfiles import awatch async def main (): async for changes in awatch ( '/path/to/dir' ): print ( changes ) asyncio . run ( main ())

See awatch docs for more details.

run_process Usage

from watchfiles import run_process def foobar ( a , b , c ): ... if __name__ == '__main__' : run_process ( './path/to/dir' , target = foobar , args = ( 1 , 2 , 3 ))

See run_process docs for more details.

arun_process Usage

import asyncio from watchfiles import arun_process def foobar ( a , b , c ): ... async def main (): await arun_process ( './path/to/dir' , target = foobar , args = ( 1 , 2 , 3 )) if __name__ == '__main__' : asyncio . run ( main ())

See arun_process docs for more details.

CLI

watchfiles also comes with a CLI for running and reloading code. To run some command when files in src change:

watchfiles "some command" src

For more information, see the CLI docs.

Or run

---

### Memecoin
 Platform Pump.fun Raises $600 Million Within 12 Minutes (3 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fgscs09/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/8Htu1fm-lDxV5d1zpWht3FnnrG8whssS91kcx0zgy1Y=413
**TLDR Summary:** Pump.fun has generated nearly $800 million in revenue since its launch in 2024.
**Full Article Content:**
In another sign of the ongoing popularity of memecoins, a platform that lets people create their own highly-speculative tokens raised $600 million within 12 minutes on Saturday.

Pump.fun’s so-called initial coin offering ranks among the largest ever and comes a day after the largest cryptocurrency and market bellwether Bitcoin hit a fresh record high.

---

### The
 Windsurf Dynamics (7 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1944406541064433848.html%3Futm_source=tldrnewsletter/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/DyTMCykKQ2L036jN9w_JAgbwNivxLiWtd0jlAHozicw=413
**TLDR Summary:** The Windsurf founders and board might have actually done the right thing by leaving a graceful path for the team.
**Full Article Content:**
The Windsurf Dynamics: On the need for a social contract, an analysis of the potential payouts / cap table math, what a better outcome might have looked like instead, and why –– maybe? –– the Windsurf founders and board might have actually done the right thing, leaving a graceful path for the team.I swear I wasn't aiming for a "twist" at the end, but I was literally researching and writing this step-by-step. I didn't rewrite it for new information, because I think all of it should be said anyway.First, I want to debunk a couple of arguments I've heard today.A. "This is consistent with waterfall / payouts according to the cap table like you'd see in an acquisition."This is not a typical acquisition, for 2 reasons:- usually in a "good" acquisition, all the employees go over to the acquirer + get new grants- they are leaving the responsibility of a company that has been gutted of key employees and leadership, for the remaining employees to deal withThis is not a "bad" outcome, obviously; there's a lot of money to go around. But because this isn't a typical acquisition (but instead a licensing deal + distributions), the mechanism to pay out proceeds according to the cap table would only cover those with vested stock, i.e., employees who have been there for >1 year. The only way that any shorter-tenured employee left behind would be taken care of, would be because the founders / board went out of their way to bat for them.A new type of exit requires a new set of norms and rules –– which we don't have yet.B. "Yeah unvested stock didn't get paid out, so what? They're adhering to the contracts."Candidates make a bunch of sacrifices by joining a startup. They are usually taking lower-than-market salary. They're taking a bunch of compensation in stock options, for the dream of a big outcome. They're taking a bet by leaving their prior jobs. They were ignoring the opportunity cost of other companies they could have joined, instead. They're probably working way harder than they would have at a public company, in exchange for impact and a stake in the outcome.In return, founders make implicit and explicit promises: we're going to win or lose, together. You're going to have skin in the game. We're going to recognize contribution, performance, and growth.This is a social contract: an understanding between people that transcends legal agreements. Bound and sealed with integrity, reputation, and duty of care. A sacred obligation between founders / execs and employees.Any breach of the social contract is a really bad thing, because it hurts the startup ecosystem overall. Startups are a high-trust endeavor. A lot of deals are done based on handshakes and assumptions of mutual good faith. If that is damaged, it is devastating for the ecosystem at large. It introduces more legalese or hedging. Less camaraderie. More transactional behavior. Less willingness to stick with it when things get tough. Silicon Valley has escaped this malaise which has beset broader corporate America.A cornerstone of this contract is founders, who sit at the intersection between investors, employees, and customers. There is an assumption that they'r'e critical to the stewardship of long-term, aligned behavior.Not all founders will, obviously... there are certainly bad apples. But bad behavior usually arises in a bad scenario. When things are good, even selfish founders take care of others, because there's so much going around. Which brings us to...Second, who's making what?I have no insider intel. But I've spent thousands of hours in a past life looking at Delaware filings, and I know how to break down a company's capital structure. So please consider the below estimates to be a very educated guess. I have tried to indicate where I feel directionally confident about estimates vs. not.A. Let's look at the current cap table. I'm reasonably confident that this section is in the right ballpark:There are currently 180-190M shares on a fully-diluted basis (after some splits I'd assume). This includes- Founder shares (estimate 80-90M or ~50%)- VC preferred stock (estimate ~75M or ~40%)- Employee option pool (estimate ~25M or ~13%)VC preferred stock likely breaks down into:- 15M Seed @ ~$0.20- 15M Series A @ ~$1.50- 20-25M Series B @ ~$3- 20-25M Series C @ ~$6.50-7.00Employee option pool likely breaks down into:- 10M issued to early employees (say, first 30, those who started in 2021, 2022, or 2023) –– each grant of 500k-1M shares for first few, down to 50-200,000 for the last few. I would guess 50-75% of that has been vested.- 5M issued to next wave of employees (say, next 100, those who started in 2024) –– each grant starting at 50-150,000 in early 2024, down to <50,000 in late 2024 after their $1.25B valuation. I would guess <20% of that has been vested.- 10M more shares probably haven't been issued and are reserved in the option pool.B. Let's look at the possible exit dynamics. I have no insight into this whatsoever, besides the above estimates + reasonable (widely ranged) estimates. Please consider this to be just a guess, for illustrative purposes, and NOT a statement of fact.We know that this is mostly about the talent, so I'd guess that a good chunk –– say $400-800M of the proceeds –– would be reserved for new retention grants. Given "dozens" of people are going over, and going by the "$100M grants" for key people that Meta is promising, let's say that the two founders each get $100-200M, and the remaining 30(?) get $1M to $10M each depending on seniority. All of this would vest, of course –– probably over 4 years.This means that the actual "exit price" would be much lower. If we think I'm anywhere close on the above figures, that would mean $1.6-2B was the portion of the exit price corresponding to the stock.Given the founders have 80-90M (fully vested), VCs have 75M, and vested stock is 5-8M, that adds up to 160-175M shares total. That would work out to $9-12 / share, so- Founders got $720M - 1.1B (plus retention grants)- VCs got $700-900M (Seed ~$150M = ~50x; Series A ~$150M = ~7x; Series B ~$200-250M = 3-4x; Series C ~$200-250M = ~1.5x)- Vested employees in total got maybe $50M (max $80M) –– heavily weighted towards first few employees who prob got $5-20M each. I'd assume that most of the early team, which would have skewed technical, is going over to Google as well (and receiving retention grants)- Holders of unvested shares got nothing, as far as we can tell –– this would correspond to roughly 7-10M more shares.Third, what should have been done differently?The simple, reputation-optimal route here would have been to ensure that employees get compensated pro-rata to the issued equity as if they had been acquired, i.e., as if the stock had been accelerated to be fully vested.This is roughly 7-10M shares, or let’s say $80-100M at the estimated exit share price.This would have required reducing the proceeds to the stock by $80-100M. VCs make ~$30-40M less, founders make ~$50-60M less. At worst, the founders could have taken the $80-100M haircut by themselves, leaving it behind to make the employees whole. After all, they're making many, many multiples of this already. In fact, isn't it kind of odd that they didn't make sure to carve out this amount –– while large?Wait...They left Windsurf with $100M in the bank, and the remaining employees are the only remaining stockholders...On the one hand, that figure reconciles with the company's fundraising ($240M raised, so $100M remaining from prior fundraising checks out).On the other hand, it's awfully similar to the unvested equity.And the company will be owned entirely by the remaining employees.They no longer have a board / VCs. They have some customers but those contracts can be refunded / wound down in an orderly fashion. I’d guess most of their revenue is MRR not ARR so the refund is probably restricted to a handful of enterprise contracts. Netting out unearned customer funds still probably puts it in the $80-100M range, aligned with the unvested equity value. That's the amount that appears to have been "not taken care of."Why... shouldn't the new management simply accelerate everyone's stock, pay out a stock dividend with the entire cash in the bank, and dissolve the company?That would be essentially equivalent economics compared to the "right" path I laid out above.If this is actually a possible path the founders & board left behind, they've actually done right by the team, considering the situation –– but, understandably, left the final decision to new management.In fact, they could not tell the management team explicitly to take these actions; if shutting down the remaining company was the golden path and decided plan all along, it would look a lot like a straight-up acquisition, which is of course subject to FTC antitrust review. This entire structure, where an operating company is left behind, has emerged just to get around the stupid FTC regime we've seen over the last 5 years. But if the new management decides to dividend + dissolve the company, well then they're simply deciding to take appropriate fiduciary steps in the context of a "tough situation" and "competitive market where we don't see a path to creating meaningful enterprise value."So... did the founders & board fail to look out for the team they’re leaving behind, or did they just leave the keys to the treasury, and trust new management to open the door? This story is full of layers. I have no idea, of course. I wasn't in the room. But the numbers seem to add up.I'm guessing we'll find out a lot more over the coming days, and see if the final chapter is determined on a spreadsheet or by the social contract.

---

### Measuring
 the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity (6 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F5CHMfP/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/ih0ukRxZ292rjrj30z2fDpwnRTAkqHpV_msVfNqdOpw=413
**TLDR Summary:** Getting a significant productivity boost from AI assistance may have a much steeper learning curve than most people expect.
**Full Article Content:**
[Scraping failed for this URL. Error: Article `download()` failed with 403 Client Error: Forbidden for url: https://simonwillison.net/2025/Jul/12/ai-open-source-productivity/?utm_source=tldrnewsletter on URL https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2F5CHMfP/1/0100019808889a82-11ba79ae-8ab6-4cd5-95b2-effbbc23e307-000000/ih0ukRxZ292rjrj30z2fDpwnRTAkqHpV_msVfNqdOpw=413]

---

