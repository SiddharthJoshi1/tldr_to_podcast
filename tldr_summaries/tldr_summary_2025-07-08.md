# TLDR Newsletter Summary: 2025-07-08

## TLDR2025-07-08

### 3
 things venture capital firms look for (Sponsor)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Finvest.pacaso.com%2F%3Futm_source=email%26utm_medium=paid-partnership%26utm_campaign=partnership30-362_07-08_10758330809/2/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/kf5DntAAgHBAdSo5JQDt2sT2cYlqvqXF4B8rjwaXmu4=412
**TLDR Summary:** For VCs, the best investments check three boxes: a massive market, a clear pain point, and founder-market fit. No wonder prominent firms like Maveron backed Pacaso. In the $1.3 trillion vacation home market, second homes were either too expensive or too much hassle. So after successfully exiting his previous business, a proven real estate founder applied his proptech expertise , selling fractions of homes and managing everything through streamlined tech. That approach has earned Pacaso $110m+ in gross profits to date, including 41% YoY growth last year alone, across 2,000+ happy homeowners. Join major VCs like Greycroft as a Pacaso investor for just $2.90/share.
**Full Article Content:**
Amazing experience with Pacaso



When we first heard about Pacaso, we already owned a vacation home and we were finding that the cost and maintenance were too cumbersome. We came across a great opportunity in Saint Helena through Pacaso, and it was perfect. It's been great to actually connect more with our friends and family. We love being a part of the community. We actually have our daughter in some summer camps in the area to make friends with other kids in the community.There's a whole team at Pacaso taking care of things. They're making sure the home is clean and maintained. They're making sure that the electric bills are being paid, the Internet bill, etc. But we still have this asset that is growing in value and we get to use it at the same time. Everything is so transparent, either in the app or from our property manager. I've actually taken some of the learnings from Picasso and implemented it in my own business because the customer service has been 5-star.



---

## Big Tech & Startups

### Alphabet's
 Isomorphic Labs has grand ambitions to ‘solve all diseases' with AI. Now, it's gearing up for its first human trials (3 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.msn.com%2Fen-us%2Fhealth%2Fother%2Falphabet-s-isomorphic-labs-has-grand-ambitions-to-solve-all-diseases-with-ai-now-it-s-gearing-up-for-its-first-human-trials%2Far-AA1I44pq%3Futm_source=tldrnewsletter/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/qj0LdLEKTiLOSvMhPU3cih8YSLP1sL6ueHFAqk5YBD8=412
**TLDR Summary:** Isomorphic Labs, Alphabet's secretive drug discovery arm, is preparing to launch human trials of AI-designed drugs. The company was born from one of DeepMind's most celebrated breakthroughs, AlphaFold, an AI system capable of predicting protein structures and interactions with a high level of accuracy. The system has helped researchers design medicines faster and more precisely.
**Full Article Content:**


---

### Alphabet's
 Isomorphic Labs has grand ambitions to ‘solve all diseases' with AI. Now, it's gearing up for its first human trials (3 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.msn.com%2Fen-us%2Fhealth%2Fother%2Falphabet-s-isomorphic-labs-has-grand-ambitions-to-solve-all-diseases-with-ai-now-it-s-gearing-up-for-its-first-human-trials%2Far-AA1I44pq%3Futm_source=tldrnewsletter/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/qj0LdLEKTiLOSvMhPU3cih8YSLP1sL6ueHFAqk5YBD8=412
**TLDR Summary:** Isomorphic Labs, Alphabet's secretive drug discovery arm, is preparing to launch human trials of AI-designed drugs. The company was born from one of DeepMind's most celebrated breakthroughs, AlphaFold, an AI system capable of predicting protein structures and interactions with a high level of accuracy. The system has helped researchers design medicines faster and more precisely.
**Full Article Content:**


---

### Jack
 Dorsey launches a WhatsApp messaging rival built on Bluetooth (3 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.cnbc.com%2F2025%2F07%2F07%2Fjack-dorsey-whatsapp-bluetooth.html%3Futm_source=tldrnewsletter/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/gv9Y-Kab0mJ6FF3ynQ8AELOKgYJ6yE6WdA-_BtmobQ0=412
**TLDR Summary:** Bitchat is a decentralized, peer-to-peer messaging app that works entirely over Bluetooth mesh networks - no internet, central servers, phone numbers, or emails required. It enables ephemeral, encrypted communication - nearby devices form clusters and pass messages from device to device, allowing them to reach peers beyond standard range, even without Wi-Fi or cell service. Messages are stored only on-device, disappear by default, and never touch centralized infrastructure. A beta version of Bitchat is currently live on TestFlight.
**Full Article Content:**
Twitter CEO Jack Dorsey testifies during a remote video hearing held by subcommittees of the U.S. House of Representatives Energy and Commerce Committee on "Social Media's Role in Promoting Extremism and Misinformation" in Washington, March 25, 2021.

Block CEO Jack Dorsey spent the weekend building Bitchat, a new decentralized, peer-to-peer messaging app that works entirely over Bluetooth mesh networks, with no internet, central servers, phone numbers or emails required.

The Twitter co-founder announced Sunday that the beta version is live on TestFlight, with a full white paper available on GitHub.

In a post on X Sunday, Dorsey called it a personal experiment in "bluetooth mesh networks, relays and store and forward models, message encryption models, and a few other things."

Bitchat enables ephemeral, encrypted communication between nearby devices. As users move through physical space, their phones form local Bluetooth clusters and pass messages from device to device, allowing them to reach peers beyond standard range — even without Wi-Fi or cell service.

Certain "bridge" devices connect overlapping clusters, expanding the mesh across greater distances. Messages are stored only on device, disappear by default and never touch centralized infrastructure — echoing Dorsey's long-running push for privacy-preserving, censorship-resistant communication.

---

## Science & Futuristic Technology

### Creating
 therapeutic abundance (22 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.jck.bio%2Fp%2Fcreating-therapeutic-abundance%3Futm_source=tldrnewsletter/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/T1Vu65t-OvkNHyIlZmwJDa-YA4nSDCyImKPGKly9fEg=412
**TLDR Summary:** The invention of new medicines is limited by our knowledge of cells and molecules that can be manipulated to treat disease. The cost of discovering new medicines is increasing. New technologies have the potential to unlock an era of target abundance and reverse the long decline in research and development productivity. This could be one of AI's most important impacts.
**Full Article Content:**
tl;dr

The invention of new medicines is rate limited by our knowledge of cells and molecules ("targets") that we can manipulate to treat disease. The cost of discovering new medicines has increased because the lowest hanging fruit has been picked on the tree of ideas. Emerging technologies at the intersection of artificial intelligence & genomics have the potential to unlock a new era of target abundance, potentially reversing the decade's long decline in R&D productivity. If realized, this will be one of the most important impacts of AI over the coming decades.

Eroom's law

Gordon Moore famously predicted in 1965 that the number of transistors per integrated circuit would double every two years. The computing industry delivered.

Jack Scannell infamously predicted in 2012 that the number of drugs per billion dollars would decline two-fold every nine years. Unfortunately, our therapeutics industry has largely followed through.

Image from Alex Telford.

Why has this happened?

Eroom's law contains within it multiple emerging problems in our industry – rising costs for R&D and declining success rates per drug program.

Rising R&D costs have many sources. A plurality likely trace back to Baumol's cost disease. Cost disease applies throughout the economy though, so on the surface, drug development's unique problems might be more directly tied to the high rate of failure for new candidate medicines.

Drug program success rates are equally complex. Failures can be attributed to safety issues, failure of a drug to hit the desired biological target, or improper selection of the target for a given disease.

Ascribing exact values to the frequency of each of these failures is challenging. Most of the knowledge of drug program lifecycles remains locked within drug companies. Nonetheless, we can bucket the failures into a two broad categories of safety and efficacy and make informed estimates.

Safety failures – ~20-30% of all candidates

A molecule was developed, but proved unsafe in patients. These are typically detected as failures in Phase 1 trials. Efficacy failures – 70-80% of all candidates

The remainder of all drug candidates that fail – 63% of all drugs placed into trials period – fail due to a lack of efficacy. Even though the drugs are safe, they don't provide benefit to the patients by treating their disease.

From these coarse numbers, it's clear that the highest leverage point in our drug development process is increasing the efficacy rate of new candidate medicines.

This fact shows up clearly in clinical trial results. The plurality of medicines fail in Phase 2 trials, the first time efficacy is measured, the first time we test the hypothesis of whether manipulating a given biological target will actually benefit patients.

Imaged from Cook et. al. 2014, Nature Reviews Drug Discovery

This stands in contrast to some rhetoric in the ecosystem claiming that an undue regulatory burden in the US market (where >50% of revenues arise) is the main challenge holding back drug development. If this were true, you'd expect to see amazing therapies that are available exclusively in ex-US geographies with simpler regulatory schemes. The absence of these medicines suggests that regulatory changes alone are insufficient to fix our therapeutic development challenge, even if they could prove an accelerant.

Rather, our main challenges are scientific. We simply don't know how to make effective drugs that preserve health or reverse disease! If we want more medicines, we need to understand why they don't work and fix it.

Why don't our candidate medicines work?

Efficacy failures can broadly occur for two reasons:

Engagement failures: We chose the right biology ("target") to manipulate, but our drug candidate failed to achieve the desired manipulation. This is the closest thing drug development has to an engineering problem. Target failures: The drug candidate manipulated our chosen biology exactly as expected. Unfortunately, the target failed to have the desired effect on the disease. This is a scientific or epistemic failure, rather than an engineering problem. We simply failed to understand the biology well enough to intervene and benefit patients.

It's difficult to know exactly the exact frequency of these two failure modes, but we can infer from a few sources that target failures dominate.

Success rates for biosimilar drugs hitting known targets are extremely high, >80%

Drugs against targets with genetic evidence have a 2-3 fold higher success rate than those against targets lacking this evidence, suggesting that picking good targets is a high source of leverage

Among organizations with meaningful internal data, picking the right target is considered the first priority of all programs (e.g. "Right target" is the first tenet of AstraZeneca's "5Rs" framework).

The predominance of target failures has likewise led most companies working on new modalities to address a small set of targets with well-validated biology. This has led to dozens of potential medicines "crowding" on the same targets, and this trend is increasing over time. A recent report from LEK demonstrates just how pronounced this trend has become. As a complement to rigorous academic and market research, simply scanning the pipeline pages of biotechs will convince an interested reader that this phenomenon is very real.

Target crowding map from LEK Consulting . It seems unlikely that this is the optimal allocation of resources if you measure “years of healthy life gained” as your objective function.

Crowding on known targets is perhaps the strongest integrated signal that target failures are the predominant reason our medicines don't work in the clinic. Many distinct teams of incredibly smart people have aggregated all information available and concluded that target discovery is so fraught, they would prefer to take on myriad market risks to avoid it.

Are targets getting harder to find?

If searching for targets is the limiting reagent in our medicine production function, the difficulty of finding targets must increase over time in order to explain part of Eroom's law. How could this be the case given all the improvements in underlying biomedical science?

In an influential paper "Are ideas getting harder to find?", Nicholas Bloom and colleagues argue that many fields of invention suffer from diminishing returns to investment. Intuitively, the low hanging fruit in a given discipline is picked early and more investment is required merely to reap the same harvest from higher branches on the tree of ideas.

In therapeutics, we can imagine concrete examples to explain how this might be the case. At the beginning of the Eroom's law data series in the 1950s, the most successful new medicines were broad spectrum antibiotics. In the 1960s and 1970s, several new medicines targeted the central dimorphic sexual hormones (estrogen and testosterone agonists and antagonists). The 1980s saw successful antivirals for HIV and early biologics for central signaling hormones (insulin, growth hormone, erythropoeitin).

It's striking from this sort of survey that infectious disease and circulating hormone targets dominated the first several decades of modern drug discovery. These targets are the most obvious examples of low hanging fruit in the industry. Infectious diseases have a small number of genes – making targets relatively easy to find – and their biology is divergent from our own, so they are uniquely straightforward to drug safely. It's easier to find a safe inhibitor of a gene if the gene only exists in a pathogen, and not normal human cells.

Hormones are likewise simple to identify because they circulate in the blood and their levels can be measured longitudinally. They are simple to drug because their structures are comparatively simple and the biology is "designed" for a single molecule to evoke a complex phenotype. Early recombinant DNA companies Genentech and Amgen both chose to develop hormone drugs because the genes were small, and therefore easier to clone and manufacture.

The common diseases that predominate as ailments today are far more complex. Targets are getting harder to find not because we are getting worse at selection, but because many of the easy and obvious therapeutic hypotheses have already been exploited.

Inventing medicines that match nature's complexity

Accelerating drug discovery will require us to discover "targets" more effectively. Not only will this involve improving our traditional target identification processes, but changing our definition of a target altogether.

Today, we typically conceive of targets as single gene or molecule that we can manipulate to achieve a therapeutic goal. This conception likely needs to be broken to access the metaphorical fruit higher on the tree.

Aging and disease involve the complex interplay of molecular circuits. Outside of infectious and inherited monogenic diseases, there are few health problems that arise as the result of a single molecule that is too high or too low in abundance. Preserving health and enhancing our physiology will require us to match the complexity of our biology with the complexity of our medicines. We need to stop thinking about targets as single molecules and begin to imagine therapeutic hypotheses that rely on combinations of genes, engineered cellular behaviors, and remodeling of tissues.

This point seems obvious. Why haven't we developed medicines like this to date?

The origins of our contemporary targets

Most of our current targets emerged from a stochastic research process. Namely, academic researchers explore the biology of a disease, then eventually identify a molecule that is necessary or sufficient for the pathology to manifest. Each of these molecules are typically proposed through a heuristic process.

Concretely, a scientist sits and thinks hard about the problem, makes a guess at the responsible molecular players based on their intuition, prior art, and their new data, then tests to see if the molecule is causal. The vast majority of these hypotheses are wrong! The few that prove to be correct often become the basis of our modern target-based drug discovery process and several companies quickly launch programs to prosecute them. This approach yielded targets like PD-1, CD19, VEGFR2, and BTK within the sphere of crowded targets today.

Despite its successes, this method has a few key limitations that explain why our current targets are so tightly constrained.

The throughput of target:disease pairs tested in this fashion and the efficiency in terms of dollars per target discovered are fairly low. Given the low throughput, it's nearly impossible to test hypotheses that involve manipulating biology in a manner more complex than dialing a single target all the way up (overexpression, drug-like agonism) or all the way down (genetic knockout, drug-like inhibition). This inherently limits us to discovering targets that are far more reductionist than the actual biology we hope to manipulate.

Distilling natural experiments

The sparsity of target space has been an acknowledged problem in the industry for decades. Shortly after the conclusion of the Human Genome Project, large scale human genetic studies appeared to offer one possible answer to the problem.

Each human genome contains more than a million variants relative to the representative “reference,” genome. These variants serve as a form of natural experiment, one of the only sources of information on the effect of manipulating a given gene in humans.

Given a large number of human genomes paired with medical records, researchers can draw associations between genetic variants and human health. Variants can then be associated to genes, and researchers can discover targets that may exacerbate or prevent a given disease. This approach has successfully yielded some of the now crowded targets in today’s pantheon, including PCSK9.

A whole cohort of companies (Celera, deCODE, Incyte, Millennium, Myriad) was created to leverage this new resource. It might seem surprising at first blush that genetic methods haven’t changed the course of R&D productivity.

While promising, human genetics can only reveal a certain class of targets. The larger the effect size of a genetic variant, the less frequently it appears in the population due to selective pressure. In effect, this means that the largest effects in biology are the least likely to be discovered using human genetics. Many of the best known targets have minimal genetic signal for this reason.

Our current methods are good at discovering individual genes that associate with health, but discovering combinations of genes is nascent at best. Human genetics cannot help us discover the combinatorial medicines or gene circuits to install in a cell therapy.

Sociologically, discovering drug targets with human genetics has become something of a consensus opinion. Most large drug discovery firms have teams dedicated to this approach. This has contributed to the crowding problem, leading many firms to address the same set of targets within the constraints of genetic discovery. These medicines can certainly be impactful, but it seems unlikely that 10+ medicines targeting PCSK9 is the optimal resource allocation for patients.

Building systems of discovery

Is it possible to build a more deterministic, less constrained discovery process? Can we discover target biologies with a complexity matching the origins of disease?

Two technological revolutions argue in the affirmative. Functional genomics methods now enable us to test far more hypotheses than ever before. From the resulting data corpuses, artificial intelligence models can search otherwise intractably large hypothesis spaces, like the space of possible genetic circuits or combinatorial therapies. By performing most experiments in the world of bits rather than atoms, it’s possible to address questions that were inaccessible to a previous generation of scientists.

Functional genomics use DNA sequencing ("reading") and synthesis ("writing") technologies to parallelize experiments at the level of cells and molecules. Rather than running each experiment in a unique test tube to keep track of the conditions, experimental details are encoded in DNA basepairs within a cell or molecule, then read-out by sequencing.

In practice, this allows researchers to treat the cell as the unit of experimentation, increasing the throughput of many target discovery questions by 100-1000X. These methods aren't applicable to every target discovery problem (e.g. some pathologies only manifest across tissue systems), but they nonetheless unlock a class of putative interventions that were previously too numerous to search effectively.

It's reasonable to think about these methods as a way of making traditional "perturbation" experiments that teach us how biological systems work amenable to the multiplexing benefits of DNA sequencing. The cost of DNA sequencing is falling over time, so this provides a tailwind to our ability to discover new target biologies for therapeutics. This is just one way that solving engineering problems can accelerate progress on the distinct and more challenging scientific problems facing our industry.

Even with the best possible experimental methods, some of the most promising target biologies will never be searched exhaustively. There are a nearly infinite number of combinatorial genetic interventions we might drug, synthetic circuits we might engineer into cells, and changes in tissue composition we might engender.

Artificial intelligence models can learn general models from the data generated in functional genomics experiments of many flavors, predicting outcomes for the experiments we haven't yet run. If we manage to construct a performant model for a given class of target biologies, we may be able to increase the efficiency of target discovery by many orders-of-magnitude. The cost of discovering a target could conceivably go from >$1B to <$1M.

There's growing interest in the idea of combining these technologies to build "virtual cells," models that can predict the outcomes of target discovery experiments in silico before they're ever executed in the lab. The grand version of this vision spans all possible target biologies, from gene inhibitions to polypharmaceutical small molecule treatments. In the maximal form, it may take many years to realize.

More limited realizations though are tractable today. The initial versions of these models are already emerging within early Predictive Biology companies. As a few examples, Recursion is building models of genetic perturbations in cancer cells, Tahoe Tx is building models in oncology with a chemical biology approach, and NewLimit has developed models for reprogramming cell age across human cell types. Focused models like these represent an early demonstration that this general approach can yield therapeutic value.

These technologies have only emerged in the last 5-10 years. This may seem like old news from an academic perspective, but drug discovery cycles are on the order of a decade. We are only now beginning to reap the first harvest from this approach. We've begun to see the first medicines addressing emerging target biologies in the clinic, including complex cell states and combinatorial nucleic acid interventions.

I'm hopeful that our ability to discover these complex target biologies will match our newfound skill in drugging them.

An era of target abundance

The data are quite compelling that target discovery is the limiting reagent in modern drug development. New technologies offer an opportunity to invert the curve of Eroom's law and arc toward progress. We have the potential to enter a future where targets are no longer rate limiting.

How should we allocate resources in light of this opportunity?

Science and therapeutic discovery are driven by pools of public (~$50B/year, US NIH + NSF), philanthropic ($1-2B/year), and private capital (~$5-10B/year, venture + IPOs). Of these, public financing is potentially the largest driver based on shear scale.

Philanthropic academic institutions (Arc, Broad, CZI) have already taken the first steps to pull this possible future forward. Both Arc and CZI have announced major initiatives to build models suitable for large scale target discovery, and the Broad recently launched an AI center that may engender similar progress.

Therapeutic discovery would benefit from public investment following suit. This will require institutions like the NIH to fund larger, team-oriented projects with more integrated support from computer science researchers than the traditional one PI, one R01 scheme that dominates the agency.

Private capital has begun to place the bets on this thesis, but a plurality of resources are still concentrated on prosecuting known targets. Even on the frontier of firms leveraging artificial intelligence (techbio firms, if you'll allow it), much capital is focused on designing new molecules to these old targets more expeditiously.

R&D investment by target class from LEK Consulting. New targets are a small subset of the light greet category (<10 associated drugs per target), representing «32% of total drugs in the pipeline.

This likely stems from the fact that while therapeutic engineering has a lower expected value than prosecuting new targets, it likewise has lower volatility, and there are larger pools of capital available for low vol, low EV bets than high vol, high EV bets.

Biotechnology companies often take decades to turn a profit. If you believe that the future of human health lies outside the narrow universe of known targets, it's rationale to allocate more resources in the direction of that emerging future, even if you believe it will take time to manifest.

Coda

Eroom’s law hangs heavily upon the neck of the biotech industry. Many have internalized it as if a form of gravity — immutable & recalcitrant to a fundamental understanding. In fact, it is neither. The slow down in R&D productivity over the past decades is primarily a limitation of our biological understanding, not the loss of a rare and essential element from the surface of the Earth or an impenetrable barrier of regulation.

Our industry has often reacted to this sense of inevitable decay by attempting to hide from risk. Rather than learning to ask better scientific questions, we’ve too often avoided asking any questions where the answers are not already known. This has resulted in hundreds of distinct therapies attempting to drug the same small group of biologies. It seems self-evident that this is not the allocation of resources that maximizes for the number of healthy years we deliver to the world.

We are entering an epoch of abundant intelligence. With these tools, we have the opportunity to discover & design target biologies at a rate that’s too cheap to meter. The therapies that emerge could serve as the counterexample that downgrades Eroom’s law to a historic conjecture.

If realized, the reignition our therapeutic discovery cadence would represent perhaps the most valuable output of the Intelligence Revolution now being rendered. There is no product more valuable than healthy time.

---

### Creating
 therapeutic abundance (22 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fblog.jck.bio%2Fp%2Fcreating-therapeutic-abundance%3Futm_source=tldrnewsletter/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/T1Vu65t-OvkNHyIlZmwJDa-YA4nSDCyImKPGKly9fEg=412
**TLDR Summary:** The invention of new medicines is limited by our knowledge of cells and molecules that can be manipulated to treat disease. The cost of discovering new medicines is increasing. New technologies have the potential to unlock an era of target abundance and reverse the long decline in research and development productivity. This could be one of AI's most important impacts.
**Full Article Content:**
tl;dr

The invention of new medicines is rate limited by our knowledge of cells and molecules ("targets") that we can manipulate to treat disease. The cost of discovering new medicines has increased because the lowest hanging fruit has been picked on the tree of ideas. Emerging technologies at the intersection of artificial intelligence & genomics have the potential to unlock a new era of target abundance, potentially reversing the decade's long decline in R&D productivity. If realized, this will be one of the most important impacts of AI over the coming decades.

Eroom's law

Gordon Moore famously predicted in 1965 that the number of transistors per integrated circuit would double every two years. The computing industry delivered.

Jack Scannell infamously predicted in 2012 that the number of drugs per billion dollars would decline two-fold every nine years. Unfortunately, our therapeutics industry has largely followed through.

Image from Alex Telford.

Why has this happened?

Eroom's law contains within it multiple emerging problems in our industry – rising costs for R&D and declining success rates per drug program.

Rising R&D costs have many sources. A plurality likely trace back to Baumol's cost disease. Cost disease applies throughout the economy though, so on the surface, drug development's unique problems might be more directly tied to the high rate of failure for new candidate medicines.

Drug program success rates are equally complex. Failures can be attributed to safety issues, failure of a drug to hit the desired biological target, or improper selection of the target for a given disease.

Ascribing exact values to the frequency of each of these failures is challenging. Most of the knowledge of drug program lifecycles remains locked within drug companies. Nonetheless, we can bucket the failures into a two broad categories of safety and efficacy and make informed estimates.

Safety failures – ~20-30% of all candidates

A molecule was developed, but proved unsafe in patients. These are typically detected as failures in Phase 1 trials. Efficacy failures – 70-80% of all candidates

The remainder of all drug candidates that fail – 63% of all drugs placed into trials period – fail due to a lack of efficacy. Even though the drugs are safe, they don't provide benefit to the patients by treating their disease.

From these coarse numbers, it's clear that the highest leverage point in our drug development process is increasing the efficacy rate of new candidate medicines.

This fact shows up clearly in clinical trial results. The plurality of medicines fail in Phase 2 trials, the first time efficacy is measured, the first time we test the hypothesis of whether manipulating a given biological target will actually benefit patients.

Imaged from Cook et. al. 2014, Nature Reviews Drug Discovery

This stands in contrast to some rhetoric in the ecosystem claiming that an undue regulatory burden in the US market (where >50% of revenues arise) is the main challenge holding back drug development. If this were true, you'd expect to see amazing therapies that are available exclusively in ex-US geographies with simpler regulatory schemes. The absence of these medicines suggests that regulatory changes alone are insufficient to fix our therapeutic development challenge, even if they could prove an accelerant.

Rather, our main challenges are scientific. We simply don't know how to make effective drugs that preserve health or reverse disease! If we want more medicines, we need to understand why they don't work and fix it.

Why don't our candidate medicines work?

Efficacy failures can broadly occur for two reasons:

Engagement failures: We chose the right biology ("target") to manipulate, but our drug candidate failed to achieve the desired manipulation. This is the closest thing drug development has to an engineering problem. Target failures: The drug candidate manipulated our chosen biology exactly as expected. Unfortunately, the target failed to have the desired effect on the disease. This is a scientific or epistemic failure, rather than an engineering problem. We simply failed to understand the biology well enough to intervene and benefit patients.

It's difficult to know exactly the exact frequency of these two failure modes, but we can infer from a few sources that target failures dominate.

Success rates for biosimilar drugs hitting known targets are extremely high, >80%

Drugs against targets with genetic evidence have a 2-3 fold higher success rate than those against targets lacking this evidence, suggesting that picking good targets is a high source of leverage

Among organizations with meaningful internal data, picking the right target is considered the first priority of all programs (e.g. "Right target" is the first tenet of AstraZeneca's "5Rs" framework).

The predominance of target failures has likewise led most companies working on new modalities to address a small set of targets with well-validated biology. This has led to dozens of potential medicines "crowding" on the same targets, and this trend is increasing over time. A recent report from LEK demonstrates just how pronounced this trend has become. As a complement to rigorous academic and market research, simply scanning the pipeline pages of biotechs will convince an interested reader that this phenomenon is very real.

Target crowding map from LEK Consulting . It seems unlikely that this is the optimal allocation of resources if you measure “years of healthy life gained” as your objective function.

Crowding on known targets is perhaps the strongest integrated signal that target failures are the predominant reason our medicines don't work in the clinic. Many distinct teams of incredibly smart people have aggregated all information available and concluded that target discovery is so fraught, they would prefer to take on myriad market risks to avoid it.

Are targets getting harder to find?

If searching for targets is the limiting reagent in our medicine production function, the difficulty of finding targets must increase over time in order to explain part of Eroom's law. How could this be the case given all the improvements in underlying biomedical science?

In an influential paper "Are ideas getting harder to find?", Nicholas Bloom and colleagues argue that many fields of invention suffer from diminishing returns to investment. Intuitively, the low hanging fruit in a given discipline is picked early and more investment is required merely to reap the same harvest from higher branches on the tree of ideas.

In therapeutics, we can imagine concrete examples to explain how this might be the case. At the beginning of the Eroom's law data series in the 1950s, the most successful new medicines were broad spectrum antibiotics. In the 1960s and 1970s, several new medicines targeted the central dimorphic sexual hormones (estrogen and testosterone agonists and antagonists). The 1980s saw successful antivirals for HIV and early biologics for central signaling hormones (insulin, growth hormone, erythropoeitin).

It's striking from this sort of survey that infectious disease and circulating hormone targets dominated the first several decades of modern drug discovery. These targets are the most obvious examples of low hanging fruit in the industry. Infectious diseases have a small number of genes – making targets relatively easy to find – and their biology is divergent from our own, so they are uniquely straightforward to drug safely. It's easier to find a safe inhibitor of a gene if the gene only exists in a pathogen, and not normal human cells.

Hormones are likewise simple to identify because they circulate in the blood and their levels can be measured longitudinally. They are simple to drug because their structures are comparatively simple and the biology is "designed" for a single molecule to evoke a complex phenotype. Early recombinant DNA companies Genentech and Amgen both chose to develop hormone drugs because the genes were small, and therefore easier to clone and manufacture.

The common diseases that predominate as ailments today are far more complex. Targets are getting harder to find not because we are getting worse at selection, but because many of the easy and obvious therapeutic hypotheses have already been exploited.

Inventing medicines that match nature's complexity

Accelerating drug discovery will require us to discover "targets" more effectively. Not only will this involve improving our traditional target identification processes, but changing our definition of a target altogether.

Today, we typically conceive of targets as single gene or molecule that we can manipulate to achieve a therapeutic goal. This conception likely needs to be broken to access the metaphorical fruit higher on the tree.

Aging and disease involve the complex interplay of molecular circuits. Outside of infectious and inherited monogenic diseases, there are few health problems that arise as the result of a single molecule that is too high or too low in abundance. Preserving health and enhancing our physiology will require us to match the complexity of our biology with the complexity of our medicines. We need to stop thinking about targets as single molecules and begin to imagine therapeutic hypotheses that rely on combinations of genes, engineered cellular behaviors, and remodeling of tissues.

This point seems obvious. Why haven't we developed medicines like this to date?

The origins of our contemporary targets

Most of our current targets emerged from a stochastic research process. Namely, academic researchers explore the biology of a disease, then eventually identify a molecule that is necessary or sufficient for the pathology to manifest. Each of these molecules are typically proposed through a heuristic process.

Concretely, a scientist sits and thinks hard about the problem, makes a guess at the responsible molecular players based on their intuition, prior art, and their new data, then tests to see if the molecule is causal. The vast majority of these hypotheses are wrong! The few that prove to be correct often become the basis of our modern target-based drug discovery process and several companies quickly launch programs to prosecute them. This approach yielded targets like PD-1, CD19, VEGFR2, and BTK within the sphere of crowded targets today.

Despite its successes, this method has a few key limitations that explain why our current targets are so tightly constrained.

The throughput of target:disease pairs tested in this fashion and the efficiency in terms of dollars per target discovered are fairly low. Given the low throughput, it's nearly impossible to test hypotheses that involve manipulating biology in a manner more complex than dialing a single target all the way up (overexpression, drug-like agonism) or all the way down (genetic knockout, drug-like inhibition). This inherently limits us to discovering targets that are far more reductionist than the actual biology we hope to manipulate.

Distilling natural experiments

The sparsity of target space has been an acknowledged problem in the industry for decades. Shortly after the conclusion of the Human Genome Project, large scale human genetic studies appeared to offer one possible answer to the problem.

Each human genome contains more than a million variants relative to the representative “reference,” genome. These variants serve as a form of natural experiment, one of the only sources of information on the effect of manipulating a given gene in humans.

Given a large number of human genomes paired with medical records, researchers can draw associations between genetic variants and human health. Variants can then be associated to genes, and researchers can discover targets that may exacerbate or prevent a given disease. This approach has successfully yielded some of the now crowded targets in today’s pantheon, including PCSK9.

A whole cohort of companies (Celera, deCODE, Incyte, Millennium, Myriad) was created to leverage this new resource. It might seem surprising at first blush that genetic methods haven’t changed the course of R&D productivity.

While promising, human genetics can only reveal a certain class of targets. The larger the effect size of a genetic variant, the less frequently it appears in the population due to selective pressure. In effect, this means that the largest effects in biology are the least likely to be discovered using human genetics. Many of the best known targets have minimal genetic signal for this reason.

Our current methods are good at discovering individual genes that associate with health, but discovering combinations of genes is nascent at best. Human genetics cannot help us discover the combinatorial medicines or gene circuits to install in a cell therapy.

Sociologically, discovering drug targets with human genetics has become something of a consensus opinion. Most large drug discovery firms have teams dedicated to this approach. This has contributed to the crowding problem, leading many firms to address the same set of targets within the constraints of genetic discovery. These medicines can certainly be impactful, but it seems unlikely that 10+ medicines targeting PCSK9 is the optimal resource allocation for patients.

Building systems of discovery

Is it possible to build a more deterministic, less constrained discovery process? Can we discover target biologies with a complexity matching the origins of disease?

Two technological revolutions argue in the affirmative. Functional genomics methods now enable us to test far more hypotheses than ever before. From the resulting data corpuses, artificial intelligence models can search otherwise intractably large hypothesis spaces, like the space of possible genetic circuits or combinatorial therapies. By performing most experiments in the world of bits rather than atoms, it’s possible to address questions that were inaccessible to a previous generation of scientists.

Functional genomics use DNA sequencing ("reading") and synthesis ("writing") technologies to parallelize experiments at the level of cells and molecules. Rather than running each experiment in a unique test tube to keep track of the conditions, experimental details are encoded in DNA basepairs within a cell or molecule, then read-out by sequencing.

In practice, this allows researchers to treat the cell as the unit of experimentation, increasing the throughput of many target discovery questions by 100-1000X. These methods aren't applicable to every target discovery problem (e.g. some pathologies only manifest across tissue systems), but they nonetheless unlock a class of putative interventions that were previously too numerous to search effectively.

It's reasonable to think about these methods as a way of making traditional "perturbation" experiments that teach us how biological systems work amenable to the multiplexing benefits of DNA sequencing. The cost of DNA sequencing is falling over time, so this provides a tailwind to our ability to discover new target biologies for therapeutics. This is just one way that solving engineering problems can accelerate progress on the distinct and more challenging scientific problems facing our industry.

Even with the best possible experimental methods, some of the most promising target biologies will never be searched exhaustively. There are a nearly infinite number of combinatorial genetic interventions we might drug, synthetic circuits we might engineer into cells, and changes in tissue composition we might engender.

Artificial intelligence models can learn general models from the data generated in functional genomics experiments of many flavors, predicting outcomes for the experiments we haven't yet run. If we manage to construct a performant model for a given class of target biologies, we may be able to increase the efficiency of target discovery by many orders-of-magnitude. The cost of discovering a target could conceivably go from >$1B to <$1M.

There's growing interest in the idea of combining these technologies to build "virtual cells," models that can predict the outcomes of target discovery experiments in silico before they're ever executed in the lab. The grand version of this vision spans all possible target biologies, from gene inhibitions to polypharmaceutical small molecule treatments. In the maximal form, it may take many years to realize.

More limited realizations though are tractable today. The initial versions of these models are already emerging within early Predictive Biology companies. As a few examples, Recursion is building models of genetic perturbations in cancer cells, Tahoe Tx is building models in oncology with a chemical biology approach, and NewLimit has developed models for reprogramming cell age across human cell types. Focused models like these represent an early demonstration that this general approach can yield therapeutic value.

These technologies have only emerged in the last 5-10 years. This may seem like old news from an academic perspective, but drug discovery cycles are on the order of a decade. We are only now beginning to reap the first harvest from this approach. We've begun to see the first medicines addressing emerging target biologies in the clinic, including complex cell states and combinatorial nucleic acid interventions.

I'm hopeful that our ability to discover these complex target biologies will match our newfound skill in drugging them.

An era of target abundance

The data are quite compelling that target discovery is the limiting reagent in modern drug development. New technologies offer an opportunity to invert the curve of Eroom's law and arc toward progress. We have the potential to enter a future where targets are no longer rate limiting.

How should we allocate resources in light of this opportunity?

Science and therapeutic discovery are driven by pools of public (~$50B/year, US NIH + NSF), philanthropic ($1-2B/year), and private capital (~$5-10B/year, venture + IPOs). Of these, public financing is potentially the largest driver based on shear scale.

Philanthropic academic institutions (Arc, Broad, CZI) have already taken the first steps to pull this possible future forward. Both Arc and CZI have announced major initiatives to build models suitable for large scale target discovery, and the Broad recently launched an AI center that may engender similar progress.

Therapeutic discovery would benefit from public investment following suit. This will require institutions like the NIH to fund larger, team-oriented projects with more integrated support from computer science researchers than the traditional one PI, one R01 scheme that dominates the agency.

Private capital has begun to place the bets on this thesis, but a plurality of resources are still concentrated on prosecuting known targets. Even on the frontier of firms leveraging artificial intelligence (techbio firms, if you'll allow it), much capital is focused on designing new molecules to these old targets more expeditiously.

R&D investment by target class from LEK Consulting. New targets are a small subset of the light greet category (<10 associated drugs per target), representing «32% of total drugs in the pipeline.

This likely stems from the fact that while therapeutic engineering has a lower expected value than prosecuting new targets, it likewise has lower volatility, and there are larger pools of capital available for low vol, low EV bets than high vol, high EV bets.

Biotechnology companies often take decades to turn a profit. If you believe that the future of human health lies outside the narrow universe of known targets, it's rationale to allocate more resources in the direction of that emerging future, even if you believe it will take time to manifest.

Coda

Eroom’s law hangs heavily upon the neck of the biotech industry. Many have internalized it as if a form of gravity — immutable & recalcitrant to a fundamental understanding. In fact, it is neither. The slow down in R&D productivity over the past decades is primarily a limitation of our biological understanding, not the loss of a rare and essential element from the surface of the Earth or an impenetrable barrier of regulation.

Our industry has often reacted to this sense of inevitable decay by attempting to hide from risk. Rather than learning to ask better scientific questions, we’ve too often avoided asking any questions where the answers are not already known. This has resulted in hundreds of distinct therapies attempting to drug the same small group of biologies. It seems self-evident that this is not the allocation of resources that maximizes for the number of healthy years we deliver to the world.

We are entering an epoch of abundant intelligence. With these tools, we have the opportunity to discover & design target biologies at a rate that’s too cheap to meter. The therapies that emerge could serve as the counterexample that downgrades Eroom’s law to a historic conjecture.

If realized, the reignition our therapeutic discovery cadence would represent perhaps the most valuable output of the Intelligence Revolution now being rendered. There is no product more valuable than healthy time.

---

### Nuclear
 reactors smaller than a semi truck to be tested in Idaho (4 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.theregister.com%2F2025%2F07%2F07%2Ftrailer_sized_microreactors%2F%3Futm_source=tldrnewsletter/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/598tdM8fAce8PGEK_qEz9Wrc_S4qDeaOHTWNi99zV8M=412
**TLDR Summary:** The US Department of Energy (DoE) has conditionally selected Westinghouse's trailer-sized eVinci test reactor and Radiant's Kaleidos unit to conduct the first fueled microreactor experiments at its Demonstration of Microreactor Experiments (DOME) facility at Idaho National Laboratory. The DOME project considers anything under 50 MW of power to be a microreactor - the eVinci is designed to produce 5 megawatts, and the Kaleidos can output 1.2 megawatts. The DoE hopes that microreactors can be used to power small, remote sites. DOME is designed to be up and running by early 2026. It will operate one experiment at a time for six months.
**Full Article Content:**
The new nuclear age of small modular reactors may not have materialized yet, but that's not stopping the US Department of Energy from getting to work on even smaller, more modular reactors with a couple of new commercial partners.

Earlier this month, the DoE conditionally selected Westinghouse's trailer-sized eVinci test reactor and Radiant's Kaleidos unit to conduct the first fueled microreactor experiments at its Demonstration of Microreactor Experiments (DOME) facility at Idaho National Laboratory.

These experimental units are really, really small compared to other reactors, with self-contained designs from both companies no larger than a semi-truck trailer. The DOME project considers anything under 50 MW of power to be a microreactor; the eVinci is designed to only produce 5 megawatts, and the Kaleidos is limited to just 1.2 megawatts of electrical power output.

For perspective, the average US home only consumes around 30 kWh of electricity per day - so even 1.2 MW is enough for a lot of homes.

Microreactors are one step down from the small modular reactors proposed by startups like Oklo and NuScale. The Nuclear Regulatory Commission (NRC) approved NuScale's original 50 MWe SMR design in February 2023 and its uprated 77 MWe module in May 2025, but no commercial units have yet broken ground. A traditional full-sized nuclear power plant, meanwhile, averages 1 gigawatt of power output - a whopping 833 times the power of a Kaleidos microreactor.

With that in mind, the DoE hopes microreactors could be used to power small, remote sites, with an eVinci described as a possible power source for a remote datacenter. The Radiant Kaleidos, meanwhile, is described as an alternative to a diesel generator, being a similar tractor-trailer-mounted size while being able to operate for five years without needing refueling.

Concept art of a Westinghouse eVinci microreactor

"Microreactors will play a big role in expanding the use of nuclear power in the United States," said Mike Goff, the Acting Assistant Secretary for Nuclear Energy. "These DOME experiments will test new reactor designs that will be counted on in the future to reliably power our homes, military bases, and mission critical infrastructure."

DOME itself is being built out of the remnants of the retired Experimental Breeder Reactor II system in Idaho, meaning it'll be housed in a facility that's already been used to securely contain experimental nuclear projects.

As "the first microreactor test bed in the world," according to the DoE, DOME will be testing new designs for some of the smallest nuclear power reactors in the world - sure to be a risky bet and needing some good protection.

The eVinci, according to the DoE, will use a "heat-pipe cooled" design to produce up to five megawatts of power for eight years without the need for refueling. The Kaleidos, meanwhile, will rely on a high-temperature gas-cooled reactor design, the DoE says.

DOME is designed to be up and running by early 2026 with plans for the experiments to operate one at a time for six months.

Neither Westinghouse nor Radiant's designs have yet been certified for general construction by the NRC, but the DoE tests are a small first step in that direction.

"Data collected from the experiments will be used to commercialize each reactor technology," the DoE said of the DOME project's planned microreactor runs. ®

---

## Programming, Design & Data Science

### Delve
 generated $1M in pipeline from TLDR newsletter ads (Sponsor)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2Fcase-studies%2Fdelve-drives-1m-in-attributed-pipeline-52x-roi-through-tldr-ads%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=secondary07082025/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/-NJQJgLGA3DYusb8blUhq4ToRCWR_fASPuRjp1TccSQ=412
**TLDR Summary:** Delve ran 4 ads across 2 TLDR newsletters and brought in 66 leads, with a meaningful chunk from enterprise. The result: $1M in attributed pipeline and a 52x ROI. This case study breaks down their strategy, results, and example ads. Read the Delve case study.
**Full Article Content:**
“Newsletters presented themselves as a really great way to reach our target audience—founders. Founders are always looking to be sold to where they enjoy going, which tend to be places where they can be educated. They want to stay up to date on the best new tech, and they’re usually pretty willing to adopt new solutions if they make sense for them and there’s some social proof.”

---

### Delve
 generated $1M in pipeline from TLDR newsletter ads (Sponsor)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fadvertise.tldr.tech%2Fcase-studies%2Fdelve-drives-1m-in-attributed-pipeline-52x-roi-through-tldr-ads%2F%3Futm_source=tldr%26utm_medium=newsletter%26utm_campaign=secondary07082025/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/-NJQJgLGA3DYusb8blUhq4ToRCWR_fASPuRjp1TccSQ=412
**TLDR Summary:** Delve ran 4 ads across 2 TLDR newsletters and brought in 66 leads, with a meaningful chunk from enterprise. The result: $1M in attributed pipeline and a 52x ROI. This case study breaks down their strategy, results, and example ads. Read the Delve case study.
**Full Article Content:**
“Newsletters presented themselves as a really great way to reach our target audience—founders. Founders are always looking to be sold to where they enjoy going, which tend to be places where they can be educated. They want to stay up to date on the best new tech, and they’re usually pretty willing to adopt new solutions if they make sense for them and there’s some social proof.”

---

### Handling
 unique indexes on large data in PostgreSQL (10 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvolodymyrpotiichuk.com%2Fblog%2Farticles%2Funique-indexes-on-large-data-in-postgres-sql%3Futm_source=tldrnewsletter/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/46KzqMcsFd-UgbNLsYJv70OFFfIy-68fvOjr0H9G1vg=412
**TLDR Summary:** PostgreSQL has a limitation on unique index entries larger than 1/3 of a buffer page. The 1/3 page size limit exists to ensure efficient page splitting. To overcome the restriction, introduce a new field that stores a hash of the large field. This allows you to store really large data inside databases, even with unique constraints.
**Full Article Content:**
Handling unique indexes on large data in PostgreSQL

A month ago, during a release to production, the feature I was working on failed to deploy due to a PostgreSQL restriction. It turned out to be a surprisingly fun and comprehensive learning moment.

Backstory

I have been working on a pretty huge release where we needed to:

Add new functionality to our core feature. Remove all duplicated entities related to this feature. Set up unique index for text field to prevent new duplicates.

The first and second parts went fine, but I ran into trouble when creating the unique index. Fun fact: everything worked perfectly in the test and staging environments, but when I tried to release to production - that’s when I faced the issue:

ERROR: index row size 3456 exceeds btree version 4 maximum 2704 for index "test_big_index_big_text_key" Detail: Index row references tuple (0,4) in relation "test_big_index". Hint: Values larger than 1/3 of a buffer page cannot be indexed. Consider a function index of an MD5 hash of the value, or use full text indexing.

This message saying that I’m trying to insert pretty big row with the size of 3456 bytes into B-Tree index where the maximum size is 1/3 of PostgresSQL disk page (2700 bytes). If that sounds unfamiliar to you or you want to understand how to create unique indexes on huge data - no worries. We’ll figure out why this limit exists and how to avoid it. Let’s start with a bit of theory.

How to achieve uniqueness in database?

If you’ve ever wondered how a database enforces uniqueness, it must be able to determine whether two values are equal (comparing the row being inserted or updated against rows that already exist) - and do this efficiently. To make this work in practice, PostgreSQL provides six built-in ways to index values, allowing you to perform certain operations faster than without indexing. Let’s take a look at two of them that support equality comparisons:

B-Tree

This is the default and most commonly used index type. Supports equality and standard operators like <, >, BETWEEN etc. Built on top of B-Tree data structure which uses sorting to enable fast lookups through a binary search algorithm.

Hash

Index which support only equality check. Built on top of hash map data structure which provides average constant-time lookup for equality comparisons.

All other index types aren’t as good for strict equality comparison and are mostly used for more specific cases or complex data types. Please refer to indexes documentation for complete information.

What index to use?

From the unique indexes documentation, we’ll see that PostgreSQL only supports uniqueness enforcement using B-Tree indexes. But why is that? We just saw that at least two index types support equality comparisons and only one supports unique indexing. Let’s see why:

Feature B-Tree Index Hash Index Supports strict value comparison Yes Yes Preserves order Yes No Handles collisions No (unique keys guaranteed) Yes (collisions are everywhere) Duplicate detection Efficient by binary search algorithm and unique keys Must check full content due to collisions

The problem lies in the hash map data structure that the hash index uses. A hash map stores values in buckets, which can lead to collisions - different values producing the same hash. Because of this, the database would need to perform a full content comparison to check whether incoming value is unique, rather than just comparing the hashes. That’s why these indexes perform poorly when it comes to enforcing uniqueness. If you want to know more about hash maps, how they work under the hood and what is collisions: check this article.

PostgreSQL data storage structure

Knowing all that information from the top, it seemed like a good idea to use a unique B-Tree index. BUT… You can’t use it for large text fields - like in my case. In production, I tried to add a unique index to fields that had around 10,000 characters, and it failed. Let’s figure out why this happens:

PostgreSQL stores your data in files on disk. It works directly with those files and gives us a way to interact with them through SQL. Those files are split into 8KB pages. Each page is a fixed-size block used to store rows or index entries.

Here’s roughly how a page looks:

Page header – stores metadata about the page Item pointers – pointers to the data rows Free space – reserved for future updates or inserts Data rows – the actual data we store Special area – (used in specific pages, like index pages, for storing additional metadata)

Here is the representation of layout from the official documentation:

This structure is interesting because PostgreSQL stores everything in 8KB pages - but we can still store field values much larger than that. So how does it work? The answer is TOAST table.

TOAST table

The TOAST stands for “The Oversized-Attribute Storage Technique”. It’s an automatic system that stores large column values in a separate TOAST table, when they don’t fit on a single 8KB page. This happens behind the scenes - you won’t usually notice it unless you’re digging deep like us right now.

You can look for TOAST table by this command:

SELECT reltoastrelid::regclass AS toast_table FROM pg_class WHERE relname = 'project_angle_questions'; SELECT * from <toast_table_from_above>;

Here we can see binary data that was separated from the actual PostgreSQL pages. It is stored in chunks ordered by chunk_seq , and if you combine them in the correct order, you’ll get the original data.

Problem with the large unique data

But why can’t I use a unique index on huge row data the same way I do with non-unique data - by storing it in a TOAST table?

The problem lies in sorting and PostgreSQL’s ability to perform fast random access lookups. If PostgreSQL allowed storing B-Tree index values outside the main table file (like in a separate TOAST table), then for large data rows, it would have to check the external storage each time to access the full content. That would kill performance - every insert would become an N+1 I/O operation just to confirm uniqueness.

So to keep things fast, PostgreSQL requires index entries to be stored inline, directly on the B-Tree pages. But then comes the next question: why is the limit 1/3 of the page (2.7KB) instead of just under 8KB (to leave space for metadata)?

Why 1/3 of the page is the maximum for unique B-Tree index?

Imagine this: I have a B-Tree index page with three elements, each 2.7KB. Now I’m trying to insert a fourth, also 2.7KB.

To keep the elements in order, I need to figure out where it fits - first, second, third, or fourth. But the page is already full, so PostgreSQL splits it in half and spreads the elements across two pages (two per page):

Now imagine if the maximum rows size were 4.0KB - each page could hold at most two rows. In overall it would lead to more frequent page splits and worse performance:

This is exactly why PostgreSQL’s developers chose an optimal balance between the maximum size of an index entry and the performance cost of splitting pages. They selected a size that keeps inserts fast while still allowing large values in unique indexes.

How to overcame such restriction

The solution is to introduce a new field that stores an autogenerated hash of the large field:

ALTER TABLE <table_name> ADD COLUMN text_hash TEXT GENERATED ALWAYS AS (your_hash_function(text)) STORED;

Hashing is the process of transforming data into a string that allows you to compare inputs, but you can’t reverse it to retrieve the original data. That’s exactly what we need, because we’re not interested in getting the original information back - we only need to check whether the data is identical or not.

There are several popular options for hashing - MD5 and SHA-256:

Feature MD5 SHA-256 Hash size 128 bits (16 bytes) 256 bits (32 bytes) Speed Faster Slower Collisions Exists. However, the probability is so low that you won’t notice it unless your dataset exceeds 2^63 rows No collisisons Built-in support Yes No (You need to install pgcrypto extension)

Depending on your needs (speed versus reliability), you can choose between them. In our case, we used an MD5 hash because it’s fast, built-in, and our dataset isn’t large enough yet to worry about collisions.

And then, create a plain B-Tree unique index on that text hash field:

CREATE UNIQUE INDEX CONCURRENTLY IF NOT EXISTS <table_name>_text_hash_index ON <table_name> (text_hash);

Thats basically it. In such way you can avoid such constraint and store really large data inside your database even with unique constraint.

Key takeaways

---

### Adding
 a feature because ChatGPT incorrectly thinks it exists (5 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fwww.holovaty.com%2Fwriting%2Fchatgpt-fake-feature%2F%3Futm_source=tldrnewsletter/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/rghUcTIy4uSO-MV5kZ403xK1hGtiiNP6bedN0n6P-J0=412
**TLDR Summary:** Soundslice is a service that scans sheet music from photographs so users can listen, edit, and practice. Over the past few months, the team at Soundslice started noticing that their error logs were full of ASCII tablature, a way of notating music for guitar that the service didn't support. The team discovered that ChatGPT was hallucinating instructions to users and telling them to go to Soundslice and use a feature that didn't exist. Instead of posting a notice telling users that ChatGPT was incorrect, the team decided to build the feature instead.
**Full Article Content:**
Written by Adrian Holovaty on July 7, 2025

Well, here’s a weird one.

At Soundslice, our sheet music scanner digitizes music from photographs, so you can listen, edit and practice. We continually improve the system, and I keep an eye on the error logs to see which images are getting poor results.

In the last few months, I started noticing an odd type of upload in our error logs. Instead of images like this...

...we were starting to see images like this:

Um, that’s just a screenshot of a ChatGPT session...! WTF? Obviously that’s not music notation. It’s ASCII tablature, a rather barebones way of notating music for guitar.

Our scanning system wasn’t intended to support this style of notation. Why, then, were we being bombarded with so many ASCII tab ChatGPT screenshots? I was mystified for weeks — until I messed around with ChatGPT myself and got this:

Turns out ChatGPT is telling people to go to Soundslice, create an account and import ASCII tab in order to hear the audio playback. So that explains it!

Problem is, we didn’t actually have that feature. We’ve never supported ASCII tab; ChatGPT was outright lying to people. And making us look bad in the process, setting false expectations about our service.

So that raised an interesting product question. What should we do? We’ve got a steady stream of new users who’ve been told incorrect facts about our offering. Do we slap disclaimers all over our product, saying “Ignore what ChatGPT is saying about ASCII tab support”?

We ended up deciding: what the heck, we might as well meet the market demand. So we put together a bespoke ASCII tab importer (which was near the bottom of my “Software I expected to write in 2025” list). And we changed the UI copy in our scanning system to tell people about that feature.

To my knowledge, this is the first case of a company developing a feature because ChatGPT is incorrectly telling people it exists. (Yay?) I’m sharing the story because I think it’s somewhat interesting.

My feelings on this are conflicted. I’m happy to add a tool that helps people. But I feel like our hand was forced in a weird way. Should we really be developing features in response to misinformation?

---

### Why
 I used to prefer permissive licenses and now favor copyleft (10 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvitalik.eth.limo%2Fgeneral%2F2025%2F07%2F07%2Fcopyleft.html%3Futm_source=tldrnewsletter/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/KCPXMhet1sERLsh5dqpDDc0fWx3VR-GgoWZB6pj30AU=412
**TLDR Summary:** Free open source software is usually published under one of two major categories of copyright licenses: permissive licenses, which freely share with everyone, or copyleft licenses, which freely share only with those who are also willing to freely share. Copyleft creates a large pool of code that developers can only legally use if they are willing to share the source code of anything they build on it, making it a very broad-based and neutral way of incentivizing more diffusion. It does not favor specific actors nor create roles for active parameter setting by central planners.
**Full Article Content:**


---

### Why
 I used to prefer permissive licenses and now favor copyleft (10 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fvitalik.eth.limo%2Fgeneral%2F2025%2F07%2F07%2Fcopyleft.html%3Futm_source=tldrnewsletter/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/KCPXMhet1sERLsh5dqpDDc0fWx3VR-GgoWZB6pj30AU=412
**TLDR Summary:** Free open source software is usually published under one of two major categories of copyright licenses: permissive licenses, which freely share with everyone, or copyleft licenses, which freely share only with those who are also willing to freely share. Copyleft creates a large pool of code that developers can only legally use if they are willing to share the source code of anything they build on it, making it a very broad-based and neutral way of incentivizing more diffusion. It does not favor specific actors nor create roles for active parameter setting by central planners.
**Full Article Content:**


---

### The
 Broken Microsoft Pact: Layoffs and Performance Management (9 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fdanielsada.tech%2Fblog%2Fmicrosoft-pact%2F%3Futm_source=tldrnewsletter/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/gAe15v8mBISs1N0cOZ9_8PcJ3znwNoI8GS8wR9Xr7IQ=412
**TLDR Summary:** There are stories of people who have been with Microsoft for 20 years, who grew up with the company, and who never had to worry about sudden changes. While the performance management system had its quirks, there was an understanding that if you did solid work, you'd have a place. Workers at Microsoft received below-market-rate pay in exchange for stability, a reasonable work-life balance, and the promise of no layoffs - this is no longer the case. This shift was almost expected: Microsoft's performance management challenges made layoffs a more attractive option than addressing individual performance issues systematically.
**Full Article Content:**
When you join Microsoft, you hear stories about people who’ve been there for 20 years, who grew up with the company, who never had to worry about sudden changes (maybe except for 2008). The performance management system had its quirks, but there was an understanding that if you did solid work, you’d have a place.

But something shifted. Maybe it was the market, maybe it was the pressure to stay competitive with companies that move faster and pay more. The implicit agreement that made lower compensation feel acceptable started to crack.

For decades, Microsoft operated under an unspoken agreement with its employees—what I call “the pact.” The deal was simple: We’ll pay you 20-50% below market rate, but in exchange, you get stability, reasonable work-life balance, and most importantly, no layoffs. This wasn’t written in any employee handbook, but everyone understood it. It was the Microsoft way.

For many strong performing engineers, this was an attractive trade-off. You might not get rich as quickly as your friends at startups, but you could plan your life around stability. You could get a mortgage, start a family, and sleep well knowing your job wasn’t going anywhere.

According to layoffs.fyi, Microsoft has laid off around 30,000 people in the last two years. To put this in context: that’s roughly 13% of Microsoft’s workforce, and while the entire tech industry saw significant layoffs (Meta, Google, Amazon all made major cuts), Microsoft’s situation stands out because of their historical commitment to employment stability. Many people are confused about this departure from Microsoft’s traditional approach. As a former employee, I want to share my perspective on why this shift was almost expected. While economic factors certainly contributed, Microsoft’s performance management challenges made layoffs a more attractive option than addressing individual performance issues systematically.

Microsoft’s performance management has always been atrocious on both sides of the ruthlessness/leniency scale. In the Ballmer era, the forced stack ranking of employees. Essentially a system where managers had to grade their teams on a curve, with a predetermined percentage forced into “underperforming” categories regardless of actual performance, caused people to live in this fear environment. Even if you had a team of stellar performers, someone had to be labeled as bottom 10%. This led to internal competition, backstabbing, and people gaming the system by avoiding high-performing teams or sabotaging colleagues.

This caused great attrition and unfavorable business outcomes for Microsoft. The system was so universally hated that when Satya Nadella became CEO in 2014, eliminating stack ranking was one of his first major moves. The opposite became true: it seems managers had to fight HR to get someone on their team out.

I personally saw the bad side of this. I had a co-worker who had no productive outcomes for a year, and it took a year of continued documentation to get him fired. This also caused a downstream effect on team morale, as he was still getting rewards and a salary similar to other people.

Great performance = mediocre pay?

Sadly, one of the tradeoffs you have to understand is that at the same level, over the same year, the difference in pay between someone at the highest level of performance versus someone who passably does their job does not create enough differentiation for most levels lower than principal, barring random exceptional one-off stock grants (which also don’t help retention, due to their randomness). This causes the system to not be a meritocracy, with “passing by” being the default.

Limitations and Context

I’ll acknowledge upfront that Microsoft is a massive, diverse company with thousands of managers and teams. My experiences reflect specific orgs, specific timeframes (2016-2022), and specific leadership chains. Your mileage may vary dramatically depending on your team, your manager, and your timing. I recognize that some teams and managers at Microsoft do provide strong performance differentiation, meaningful career growth, and excellent employee experiences.

However, these patterns were consistent enough across my tenure, conversations with colleagues from different orgs, and observations of broader company trends that I believe they represent systemic issues rather than isolated incidents. The goal isn’t to dismiss positive experiences but to highlight structural challenges that affect enough employees to warrant discussion.

At will employment

For me, coming from outside the US where employment contracts offer significantly more protection, at-will employment was a jarring concept. In most countries, terminating an employee requires cause, documentation, and often significant severance payments. Employment contracts typically specify notice periods — sometimes months — giving both parties time to plan transitions.

At-will employment means your employer can terminate you for any reason (or no reason) at any time, as long as it’s not discriminatory. No advance notice required. No severance guaranteed. One day you’re planning your next sprint, the next day you’re cleaning out your desk.

This reality requires a completely different approach to financial and mental planning. You can’t just budget for your monthly expenses, you need to maintain an emergency fund that can cover 6+ months of unemployment. You can’t get too comfortable or assume job security based on performance alone. You need to constantly network, keep your resume updated, and maintain relationships outside your current company.

The Layoff Paradox: Why It’s “Easier” Than Firing

Here’s where at-will employment creates a perverse incentive structure that helps explain Microsoft’s recent layoff approach. While you can fire someone for any reason, firing individual employees for performance still carries significant legal risk and administrative burden.

To fire someone for performance, managers must document everything meticulously. Every missed deadline, every subpar deliverable, every coaching conversation needs to be recorded. HR requires a paper trail that can withstand legal scrutiny. This process can take months or even years, during which the underperforming employee continues to collect their salary and potentially drag down team morale.

The legal risk is real. Even in at-will states, wrongful termination lawsuits can be expensive and time-consuming. Companies worry about discrimination claims, especially if the fired employee belongs to a protected class. Better to have an ironclad documentation trail than face a costly legal battle.

Layoffs, paradoxically, are “cleaner.” When you eliminate entire roles or teams, you sidestep the performance documentation requirements. You’re not firing someone for being bad at their job, you’re eliminating the job itself. The legal risk is minimal, and you can even look compassionate by offering severance packages and transition support. Employees can even claim unemployment and keep their income for months!

For VPs and executives, layoffs solve multiple problems at once: they can eliminate underperformers without the messy documentation process, reduce headcount to hit financial targets, and maintain the narrative that they’re making “tough but strategic decisions” rather than admitting they failed to manage performance effectively.

The bitter irony? Getting laid off is often better for the employee than being fired. Layoffs typically come with severance, extended healthcare, and the ability to say you were “affected by restructuring” rather than “terminated for cause.” You get more time to find your next job, and your professional reputation remains intact.

This creates a system where companies find it easier to fire good employees in bulk than to fire bad employees individually. The legal protections meant to prevent arbitrary termination end up enabling exactly that, just at scale.

For visa workers, the math is even more stark. When fired for performance, H-1B and other visa holders typically have just 60 days to find new employment or leave the country. But when laid off, they get severance time plus the 60-day grace period, often giving them 3-6 months to secure new sponsorship. For someone whose legal status depends on employment, layoffs are significantly more humane than performance-based termination.

The AI efficiency narrative provides the perfect cover for these layoffs. Companies can frame headcount reduction as “leveraging AI to increase productivity” or “optimizing for the future of work.” It sounds forward-thinking and strategic rather than admitting they failed to manage performance or simply want to cut costs. Whether AI actually replaces the laid-off workers’ productivity is rarely measured or proven, but the narrative sells well to investors and the media.

The pact is broken

The pact is fundamentally broken. Instead, a new pact has been formed:

Be afraid, because we’ll randomly fire any % of you. We’ll pay you less than market. You’ll have amazing benefits.

Understanding the Trade-offs

Rather than prescribing what people should do, I think it’s worth understanding the game theory at play. If exceptional performance only nets a negliglible level of improvement while layoff risk remains constant, rational actors might consider:

Focus energy strategically. When reward differentiation is minimal, investing extra energy in building external networks and transferable skills may provide better long-term returns than purely internal recognition. Maintain financial cushions. Regardless of performance level, keep emergency funds for unexpected transitions. Keep options open. Whether this means networking, skill development, or periodic market checks depends on your personal risk tolerance and career goals.

The optimal approach depends on your individual circumstances, risk tolerance, and what you value in a career.

Would I work for Microsoft today?

The Microsoft I joined no longer exists today.. The implicit contract that made lower compensation tolerable — stability and job security — has been broken. What remains is a company that pays below market rate while offering the same job insecurity as anywhere else.

For current and prospective Microsoft employees, understanding this new reality is crucial. The company still offers excellent benefits, interesting technical challenges, and the prestige of working at a tech giant. But the days of trading pay for security are over.

Would I return to Microsoft knowing what I know now? Maybe. But it would be with a completely different mindset than my first time around. I’d go in understanding that it’s a traditional job with traditional job insecurity, not the stable career haven I once believed it to be. I’d negotiate harder on compensation, knowing there’s no stability premium to justify below-market pay. And I’d never again make the mistake of believing that good performance alone guarantees job security.

The Microsoft of today can still be a great place to work, if you go in with realistic expectations and plan accordingly.

A Note on Perspective

I want to be clear about something: I’m not bitter about my time at Microsoft. I learned enormously, worked on products used by millions, built lasting relationships, and gained experience that directly contributed to my current success. Microsoft gave me opportunities I’m genuinely grateful for, and I have tremendous respect and admiration for many of the people I worked with there.

This analysis isn’t about personal grievances. It’s about a structural shift that affects how people should think about employment decisions. The same way you’d analyze any investment or major life choice, understanding the changing risk/reward profile at Microsoft (or any company) is just rational decision-making.

Many people thrive at Microsoft today and will continue to do so. The goal isn’t to discourage anyone from working there, but to help people make informed decisions based on the current reality rather than outdated assumptions about job security.

---

## Quick Links

### OpenAI
 Ramps Up Stock Pay to $4.4 billion to Combat Meta's Costly Talent Raids (3 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FTkYpjp/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/702ysLdMglpypddr03gol3XhkLc8GvufAlEyev6kAhs=412
**TLDR Summary:** OpenAI is dramatically increasing its employee stock compensation to defend against rival Meta's poaching.
**Full Article Content:**
[Scraping failed for this URL. Error: Article `download()` failed with 403 Client Error: Forbidden for url: https://winbuzzer.com/2025/07/07/openai-ramps-up-stock-pay-to-combat-metas-costly-talent-raids-xcxwbn/?utm_source=tldrnewsletter on URL https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FTkYpjp/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/702ysLdMglpypddr03gol3XhkLc8GvufAlEyev6kAhs=412]

---

### OpenAI
 Ramps Up Stock Pay to $4.4 billion to Combat Meta's Costly Talent Raids (3 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FTkYpjp/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/702ysLdMglpypddr03gol3XhkLc8GvufAlEyev6kAhs=412
**TLDR Summary:** OpenAI is dramatically increasing its employee stock compensation to defend against rival Meta's poaching.
**Full Article Content:**
[Scraping failed for this URL. Error: Article `download()` failed with 403 Client Error: Forbidden for url: https://winbuzzer.com/2025/07/07/openai-ramps-up-stock-pay-to-combat-metas-costly-talent-raids-xcxwbn/?utm_source=tldrnewsletter on URL https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FTkYpjp/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/702ysLdMglpypddr03gol3XhkLc8GvufAlEyev6kAhs=412]

---

### Waymo
 robotaxis are heading to Philadelphia and NYC (2 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Ftechcrunch.com%2F2025%2F07%2F07%2Fwaymo-heading-to-philadelphia-and-nyc%2F%3Futm_source=tldrnewsletter/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/DaB67VjDLXN6jRCo4HQex811EjCDMnPuDhjAwO5PDrI=412
**TLDR Summary:** Waymo kicked off two 'road trips' to Philadelphia and New York City yesterday - these branded 'road trips' don't necessarily signal a commercial launch, but it has happened before.
**Full Article Content:**
Waymo kicked off two “road trips” to Philadelphia and New York City on Monday, signaling the Alphabet-owned company’s interest in expanding into Northeastern cities.

Waymo’s branded “road trips” don’t necessarily signal a commercial launch anytime soon. Waymo has announced several other road trips this year, including to Houston, Orlando, Las Vegas, San Diego, and San Antonio. Typically, the trips involve sending a small fleet of human-driven vehicles equipped with Waymo’s autonomous driving system to map out the new city. Then Waymo tests the vehicles autonomously, though still with a human behind the wheel, before taking any data and learnings back to its engineers to improve the AI driver’s performance.

In some cases, these road trips have led to commercial launches. In 2023, the company made a road trip to Santa Monica, a city in Los Angeles County. The company now operates a commercial service in Los Angeles, including Santa Monica, Beverly Hills, and Hollywood.

For its Philadelphia trip, Waymo plans to place vehicles in the most complex parts of the city, including downtown and on freeways, according to a spokesperson. She noted folks will see Waymo vehicles driving “at all hours throughout various Philadelphia neighborhoods, from North Central to Eastwick, University City, and as far east as the Delaware River.”

This city is a National Treasure. It’s a city of love, where eagles fly with a gritty spirit and cheese that spreads and cheese that steaks. Our road trip continues to Philly next. https://t.co/96XzLSFV8O https://t.co/L4PkJySxO5 pic.twitter.com/G58dFGlSaa — Waymo (@Waymo) July 7, 2025

In NYC, Waymo will drive its cars manually in Manhattan just north of Central Park down to The Battery and parts of Downtown Brooklyn. The company will also map parts of Jersey City and Hoboken in New Jersey.

Waymo applied last month for a permit to test its AVs in New York City with a human behind the wheel. The company has not yet received approval.

This isn’t the company’s first time in the Big Apple. Waymo initially deployed a small fleet of vehicles in late 2021 to map parts of Manhattan and New Jersey. This past winter, Waymo took a road trip up to Buffalo to test self-driving in wintery conditions.

Techcrunch event Save up to $475 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW

Even if Waymo gets approved to test its vehicles autonomously in NYC with a specialist behind the wheel, it’ll be a long road to commercial deployment. NYC’s AV regulations don’t currently allow operators to deploy self-driving vehicles with no human in the front seat — a law Waymo is currently advocating to change.

Waymo’s continued testing across the country is anchored by the company’s current commercial robotaxi services offered in Atlanta, Austin, the Bay Area, Los Angeles, and Phoenix. Waymo plans to launch in Miami this year and Washington, D.C., in 2026.

---

### Apple
 Loses Top AI Models Executive to Meta's Hiring Spree (6 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FMxPt5w/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/x6-vroev1veSuJAQD4sZDAxSYtC8YDEfVWs-O6iZFVw=412
**TLDR Summary:** Ruoming Pang, the executive in charge of Apple's foundation models team, is departing the company for Meta, who offered Pang a package worth tens of millions of dollars per year.
**Full Article Content:**
[Scraping failed for this URL. Error: Article `download()` failed with 403 Client Error: Forbidden for url: https://www.bloomberg.com/news/articles/2025-07-07/apple-loses-its-top-ai-models-executive-to-meta-s-hiring-spree?accessToken=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb3VyY2UiOiJTdWJzY3JpYmVyR2lmdGVkQXJ0aWNsZSIsImlhdCI6MTc1MTkzNzMyMywiZXhwIjoxNzUyNTQyMTIzLCJhcnRpY2xlSWQiOiJTWjFQNE1EV1JHRzAwMCIsImJjb25uZWN0SWQiOiJFQTExNDNDNTM4NEE0RUY5QTg5RjJEN0IxMTg2MzcwOSJ9.Y2aMUzMN79HixTZyJl1DUIM7UFtrncjxSgDE3Tm10Bo&utm_source=tldrnewsletter on URL https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2FMxPt5w/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/x6-vroev1veSuJAQD4sZDAxSYtC8YDEfVWs-O6iZFVw=412]

---

### What
 Would Stripe L1 Look Like? (13 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Flinks.tldrnewsletter.com%2Fy6RVhe/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/sE1G8fzhWr1jfcqt9OW5xWaopuFAzoofkHtsRhbMr8M=412
**TLDR Summary:** A Stripe L1 should support stablecoin use and Stripe L1 integration for customer payments and merchant settlement.
**Full Article Content:**
JavaScript is not available.

We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.

Help Center

---

### Tech
 hiring is not the same as it was even a year ago (1 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Fthreadreaderapp.com%2Fthread%2F1942251177779024262.html%3Futm_source=tldrnewsletter/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/75HcOL6Jx1GHYumVzy6NqAg9yTau12UEcbWLCK9Ua8I=412
**TLDR Summary:** Hiring for new graduates in Big Tech is down 25% and -11% for startups.
**Full Article Content:**
Jul 5

Why are 6' tall well-educated high earning men AND attractive women single?



Modern dating is in a crisis for everyone. It is one of the biggest problems of our time. Let’s break it down with data.



1. Women are more educated than ever, and prefer men the same or more.

Women are getting educated at higher rates than men (47% of women 25-34 have a bachelors vs 37% of men), yet strongly prefer partners of the same education level (81% of couples where one is college-educated have both college-educated). Few men meet this criteria.



2. Women make more money than ever, and prefer men who make more.

College-educated women in cities make MORE than men (women age 25-34 earn >100% of men in 22 large US metros including NYC and DC), yet data shows they are hypergamous, meaning they'd rather date men who make more than them (only 16% of heterosexual marriages have a female breadwinner). Data also shows that women save less than men of commensurate income, meaning the man's income becomes important to keep up their lifestyle if they ever want to purchase a house or settle down. Few men meet this criteria.



3. Women's standards are heightened by dating apps, which force filtering on criteria that are superficial and not long-term focused.

Women are in short supply in dating apps (74% on Tinder, 64% of Hinge is male), and confuse the hookup market for the marriage market. They apply selective filtering given the affordances of the apps (height, looks, education, age, race) and match with men who are "out of their league" and don't want to settle with them (median users message people 10-17%ile more attractive than themselves). When they do want to settle, this group of men self-select for being incompatible on another critical dimension like values, income or personality, leaving women unhappy with the result. Good men who fall short of the initial filter come across too strong and/or feel like a backup and find it hard to love a woman who he can sense is "settling" for him.



4. Social media has polarized both genders.

Both sides have false expectations around dating. For women, they're taught to never settle for less, be an independent girlboss, and that they don't need anyone short of perfection. Men are taught that the only women worth liking having perfect Instagram bodies, are told to date women with fewer sexual partners, that women want them for money, and the world today is just against them in every way. This makes many of them isolate and not enjoy women at all. They see less eye to eye moreso than ever on politics, opinions, and shared interests as a result. The gender “war” in Korea best exemplifies this.



5. Women take longer to find a long term match, but are seen as less attractive with age.

Because of 3, women often date around longer in their most appealing and fertile years. They see a rapid fall off in their own matches after 30 (data shows women desirability peaks at 18-22 while men climbs until 50), when it becomes a race to the bottom of the declining quality of suitors that are interested and their own declining standards: "I should've just been with that one guy from 5yrs ago." When they find a guy, they want to move quickly into marriage and kids which is often too aggressive for men. Although its supposedly taboo to say but men also prefer women with less former partners especially when choosing a spouse, which makes it more difficult for these women (data shows men willingness to commit drops sharply >8 sexual partners)



6. The world is global, and paths diverge more often than before.

Men and women both are increasingly ambitious and will move cities in search of better opportunities. Long distance relationships have lower survival rates, which consume a lot of prime dating years (40% of long distances fail, with an avg separation of 4.5mos).



7. Dating advice reinforces poor behavior.

All of this is exacerbated by people on both sides who are either in a relationship or single offering unhelpful advice like "just suck it up" or "try harder" which makes men and women cement their pre-existing habits. 20%+ of people <30 say their dating guidance primarily comes from TikTok / Instagram.



8. Ambitious women don't want kids more than men. The ones that do struggle to find partners that can afford it.

64% of women under 50 without children don't want kids vs 50% for men. Men often want kids in the former case or can't really afford them in the latter (median cost in metros is $25-32k/yr for a child and 36% of adults cite cost as the major reason for no kids). Either way, it's a bad situation.



Neither women nor men are at fault for this situation society has found itself in, and this post isn’t blaming anyone in particular. The cause is an intricate combination of the state of the economy, access through dating apps, social media and innate mating preferences. If we cannot talk

---

### Unless
 users take action, Android will let Gemini access third-party apps (6 minute read)
**URL:** https://tracking.tldrnewsletter.com/CL0/https:%2F%2Farstechnica.com%2Fsecurity%2F2025%2F07%2Funless-users-take-action-android-will-let-gemini-access-third-party-apps%2F%3Futm_source=tldrnewsletter/1/01000197e9a480c2-a1805fd5-9ae8-4c52-8c50-ce27d1c211dd-000000/gJb-b4KxP6flh35tifvooqtIR924V7JawogZp9ONu_8=412
**TLDR Summary:** Google is implementing a change that will enable its Gemini AI engine to interact with third-party apps even if users have configured their devices to block such interactions.
**Full Article Content:**
Starting today, Google is implementing a change that will enable its Gemini AI engine to interact with third-party apps, such as WhatsApp, even when users previously configured their devices to block such interactions. Users who don't want their previous settings to be overridden may have to take action.

An email Google sent recently informing users of the change linked to a notification page that said that “human reviewers (including service providers) read, annotate, and process” the data Gemini accesses. The email provides no useful guidance for preventing the changes from taking effect. The email said users can block the apps that Gemini interacts with, but even in those cases, data is stored for 72 hours.

An email Google recently sent to Android users. An email Google recently sent to Android users.

No, Google, it’s not good news

The email never explains how users can fully extricate Gemini from their Android devices and seems to contradict itself on how or whether this is even possible. At one point, it says the changes “will automatically start rolling out” today and will give Gemini access to apps such as WhatsApp, Messages, and Phone “whether your Gemini apps activity is on or off.” A few sentences later, the email says, “If you have already turned these features off, they will remain off.” Nowhere in the email or the support pages it links to are Android users informed how to remove Gemini integrations completely.

Compounding the confusion, one of the linked support pages requires users to open a separate support page to learn how to control their Gemini app settings. Following the directions from a computer browser, I accessed the settings of my account’s Gemini app. I was reassured to see the text indicating no activity has been stored because I have Gemini turned off. Then again, the page also said that Gemini was “not saving activity beyond 72 hours.”

---

